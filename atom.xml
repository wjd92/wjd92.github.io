<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>BOC&#39;s warehouse</title>
  
  
  <link href="http://www.bumpchicken.cn/atom.xml" rel="self"/>
  
  <link href="http://www.bumpchicken.cn/"/>
  <updated>2022-05-11T09:37:53.447Z</updated>
  <id>http://www.bumpchicken.cn/</id>
  
  <author>
    <name>wjd</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>贝叶斯优化原理及应用</title>
    <link href="http://www.bumpchicken.cn/2020/12/29/BayesianOptimization/"/>
    <id>http://www.bumpchicken.cn/2020/12/29/BayesianOptimization/</id>
    <published>2020-12-29T04:07:00.000Z</published>
    <updated>2022-05-11T09:37:53.447Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://images.bumpchicken.cn/img/20220509122455.png"></p><p>贝叶斯优化(Bayesian Optimization,BO)是一种黑盒优化算法，用于求解表达式未知的函数极值问题。算法使用高斯过程回归对一组采样点的函数值进行概率建模，预测出任意点处函数值的概率分布，然后构造采集函数(AcquistionFunction)，用于衡量每一个点值得探索(explore)的程度，求解采集函数的极值从而确定下一个采样点，最后返回这组采样点的极值作为函数的极值。</p><span id="more"></span><h2 id="黑盒优化问题">黑盒优化问题</h2><p>训练机器学习模型过程中，会有很多模型参数之外的参数，如学习率，卷积核大小等，再比如训练xgboost时，树的最大深度、采样率等参数都会影响训练结果，这些参数我们将其称为超参数。假设一组超参数组合<spanclass="math inline">\(X=x_{1},x_{2},...,x_{n}\)</span>，存在一个未知函数<span class="math inline">\(f:x \rightarrow \mathbb{R}\)</span>，我们需要在<span class="math inline">\(x \inX\)</span>找到一组最佳参数组合<spanclass="math inline">\(x^{*}\)</span>使得:</p><p><span class="math display">\[x^{*} = \underset{x\in X}{argmin} f(x)\]</span></p><p>当<spanclass="math inline">\(f\)</span>是凸函数并且定义域也是凸的时候，可以通过凸优化手段(梯度下降、L_BFGS等)来求解。但是超参数优化属于黑盒优化问题，<spanclass="math inline">\(f\)</span>不一定是凸函数，并且是未知的，在优化过程中只能得到函数的输入和输出，不能获取目标函数的表达式和梯度信息，这里的<spanclass="math inline">\(f\)</span>通常还是计算代价非常昂贵的函数，因此优化过程会比较困难，尤其是当超参数数量大的情况。常用的超参数优化方法有网格搜索(GridSearch)，随机搜索(RandomSearch)，遗传算法（粒子群优化、模拟退火等）以及本文要介绍的贝叶斯优化方法。</p><p>下面介绍两种最基本的超参调优方法: 网格搜索法和随机搜索法</p><ul><li><p>网格搜索法</p><p>网格搜索法搜索一组离散的取值情况，得到最优参数值。如果是连续型的超参数，则需要对其定义域进行网格划分，然后选取典型值计算。网格搜索法本质上是一种穷举法，对待调优参数进行全排列组合，逐一计算<spanclass="math inline">\(f\)</span>，然后选取最小的<spanclass="math inline">\(f\)</span>时的参数组合，如下代码所示，给定参数候选项，我们可以列出所有的参数组合<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line">tuning_params = &#123;<span class="string">&#x27;a&#x27;</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], <span class="string">&#x27;b&#x27;</span>:[<span class="number">4</span>,<span class="number">5</span>]&#125;                     <span class="comment"># 待优化参数可选项</span></span><br><span class="line"><span class="keyword">for</span> conf <span class="keyword">in</span> product(*tuning_params.values()):</span><br><span class="line">    <span class="built_in">print</span>(&#123;k:v <span class="keyword">for</span> k,v <span class="keyword">in</span> <span class="built_in">zip</span>(tuning_params.keys(), conf)&#125;)  <span class="comment"># 生成参数组合</span></span><br></pre></td></tr></table></figure> 输出: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">4</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">5</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">4</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">5</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">4</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">5</span>&#125;</span><br></pre></td></tr></table></figure>随着待调优参数增加，生成的全排列组合数量将非常巨大，计算代价过于昂贵</p></li><li><p>随机搜索法相比于网格搜索法，随机搜索的做法是将超参数随机地取某些值，设置一个最大迭代次数，比较每次迭代中不同取值算法的输出，得到最优超参数组合。而随机取值的方法也有多种不同的做法，常用的做法是采用均匀分布的随机数进行搜索，或者采用一些启发式的搜索策略（粒子群优化算法），这里不展开赘述。随机搜索并不总能找到全局最优解，但是通常认为随机搜索比网格搜索更优，其可以花费更少的计算代价得到相近的结果。</p></li></ul><p><strong>无论是网格搜索法还是随机搜索法，每一次进行迭代计算的时候，都未曾考虑已经搜索过的空间，即搜索过的空间未对下一次搜索产生任何指导作用，因此可能存在很多无效搜索。不同于网格搜索和随机搜索法，贝叶斯优化则能够通过高斯过程回归有效利用先验的搜索空间进行下一次搜索参数的选择，能大大减少迭代次数</strong></p><h2 id="理论准备">理论准备</h2><p>经典的贝叶斯优化利用高斯过程(Gaussian Process, GP)对<spanclass="math inline">\(f\)</span>进行概率建模，在介绍贝叶斯优化之前，有必要了解一下高斯过程回归的相关知识</p><h3 id="高斯过程">高斯过程</h3><p>高斯过程用于对一组随着时间增长的随机向量进行建模，在任意时刻，某个向量的所有子向量均服从高斯分布。假设有连续型随机变量序列<spanclass="math inline">\(x_{1},x_{2},...,x_{T}\)</span>，如果该序列中任意数量的随机变量构成的向量<spanclass="math inline">\(X_{t_{1}, ... ,t_{k}} = [x_{t_{1}} \space ...\spacex_{t_{k}}]^{T}\)</span>均服从多维正态分布，则称次随机变量序列为高斯过程。</p><p>特别地，假设当前有k个随机变量<spanclass="math inline">\(x_{1},...,x{k}\)</span>，它们服从k维正态分布<spanclass="math inline">\(N( \mu_{k}, \sum _{k})\)</span>，其中均值向量<span class="math inline">\(N( \mu_{k}, \sum_{k} )\)</span>，协方差矩阵<span class="math inline">\(\sum _{k} \in\mathbb R^{k*k}\)</span></p><p>当加入一个新的随机变量<spanclass="math inline">\(x_{k+1}\)</span>之后，随机向量<spanclass="math inline">\(x_{1},x_{2},...,x_{k},x_{k+1}\)</span>服从k+1维正态分布<spanclass="math inline">\(\mu_{k+1} \in\mathbb{R}^{k+1}\)</span>，其中均值向量<spanclass="math inline">\(\mu_{k+1} \in\mathbb{R}^{k+1}\)</span>，协方差矩阵<span class="math inline">\(\sum_{k+1} \in \mathbb R^{(k+1)*(k+1)}\)</span></p><p>由于正态分布的积分能够得到解析解，因此可以方便地得到边缘概率于条件概率。</p><h3 id="高斯过程回归">高斯过程回归</h3><p>机器学习中，算法通常是根据输入值<spanclass="math inline">\(x\)</span>预测出一个最佳输出值<spanclass="math inline">\(y\)</span>，用于分类或回归。某些情况下我们需要的不是预测出一个函数值，而是给出这个函数值的后验概率分布<spanclass="math inline">\(p(y|x)\)</span>。对于实际应用问题，一般是给定一组样本点<spanclass="math inline">\(x_{i}, \spacei=1,…,l\)</span>，基于此拟合出一个假设函数，给定输入值<spanclass="math inline">\(x\)</span>，预测其标签值或者后验概率<spanclass="math inline">\(p(y|x)\)</span>，高斯过程回归对应后者。</p><p>高斯过程回归(Gaussian Process Regression,GPR)对表达式未知的函数的一组函数值进行概率建模，给出函数值的概率分布。嘉定给定某些点<spanclass="math inline">\(x_{i}, i=1,…,t\)</span>，以及在这些点处的函数值<spanclass="math inline">\(f(x_{i})\)</span>，GPR能够根据这些点，拟合该未知函数，那么对于任意给定的<spanclass="math inline">\(x\)</span>，就可以预测出<spanclass="math inline">\(f(x)\)</span>，并且能够给出预测结果的置信度。</p><p>GPR假设黑盒函数在各个点处的函数值<spanclass="math inline">\(f(x)\)</span>都是随机变量，它们构成的随机向量服从多维正态分布。假设有t个采样点<spanclass="math inline">\(x_{1},…,x_{t}\)</span>，在这些点处的函数值构成向量：</p><p><span class="math display">\[f(x_{1:t}) = [f(x_{1} \space ... \spacef(x_{t})]\]</span></p><p>GPR假设此向量服从t维正态分布：</p><p><span class="math display">\[f(x_{1:t}) \sim N(\mu(x_{1:t}),\sum(x_{1:t},x_{1:t}))\]</span></p><p>其中，<spanclass="math inline">\(\mu(x_{1:t})=[\mu(x_{1}),…,\mu(x_{t})]\)</span>是高斯分布的均值向量，<spanclass="math inline">\(\sum(x_{1:t},x_{1:t})\)</span>是协方差矩阵</p><p><span class="math display">\[\left[\begin{array}{ccc}\operatorname{cov}\left(\mathbf{x}_{1}, \mathbf{x}_{1}\right) &amp;\ldots &amp; \operatorname{cov}\left(\mathbf{x}_{1},\mathbf{x}_{t}\right) \\\cdots &amp; \ldots &amp; \ldots \\\operatorname{cov}\left(\mathbf{x}_{t}, \mathbf{x}_{1}\right) &amp;\ldots &amp; \operatorname{cov}\left(\mathbf{x}_{t},\mathbf{x}_{t}\right)\end{array}\right]=\left[\begin{array}{ccc}k\left(\mathbf{x}_{1}, \mathbf{x}_{1}\right) &amp; \ldots &amp;k\left(\mathbf{x}_{1}, \mathbf{x}_{t}\right) \\\ldots &amp; \ldots &amp; \ldots \\k\left(\mathbf{x}_{t}, \mathbf{x}_{1}\right) &amp; \ldots &amp;k\left(\mathbf{x}_{t}, \mathbf{x}_{t}\right)\end{array}\right]\]</span></p><p>问题的关键是如何根据样本值计算出正态分布的均值向量和协方差矩阵，均值向量是通过均值函数<spanclass="math inline">\(\mu(x)\)</span>根据每个采样点x计算构造的，可简单令<spanclass="math inline">\(\mu(x)=c\)</span>，或者将均值设置为0，因为即使均值设置为常数，由于有方差的作用，依然能够对数据进行有效建模。</p><p>协方差通过核函数<spanclass="math inline">\(k(x,x^{&#39;})\)</span>计算得到，也称为协方差函数，协方差函数需要满足以下要求：</p><ol type="1"><li>距离相近的样本点<span class="math inline">\(x\)</span>和<spanclass="math inline">\(x^{&#39;}\)</span>之间有更大的正协方差值，因为相近的两个点的函数值有更强的相关性</li><li>保证协方差矩阵是对称半正定矩阵</li></ol><p>常用的是高斯核和Matern核，高斯核定义为：</p><p><span class="math display">\[k\left(\mathbf{x}_{1},\mathbf{x}_{2}\right)=\alpha_{0} \exp \left(-\frac{1}{2\sigma^{2}}\left\|\mathbf{x}_{1}-\mathbf{x}_{2}\right\|^{2}\right)\]</span></p><p>Matern核定义为:</p><p><span class="math display">\[k\left(\mathbf{x}_{1},\mathbf{x}_{2}\right)=\frac{2^{1-v}}{\Gamma(v)}\left(\sqrt{2v}\left\|\mathbf{x}_{1}-\mathbf{x}_{2}\right\|\right)^{v}K_{v}\left(\sqrt{2v}\left\|\mathbf{x}_{1}-\mathbf{x}_{2}\right\|\right)\]</span></p><p>其中<span class="math inline">\(\Gamma\)</span>是伽马函数，<spanclass="math inline">\(K_{v}\)</span>是贝塞尔函数(Bessel function)，<spanclass="math inline">\(v\)</span>是人工设定的正参数。用核函数计算任意两点之间的核函数值，得到核函数矩阵<spanclass="math inline">\(K\)</span>作为协方差矩阵的估计值：</p><p><span class="math display">\[\mathbf{K}=\left[\begin{array}{ccc}k\left(\mathbf{x}_{1}, \mathbf{x}_{1}\right) &amp; \ldots &amp;k\left(\mathbf{x}_{1}, \mathbf{x}_{t}\right) \\\ldots &amp; \ldots &amp; \ldots \\k\left(\mathbf{x}_{t}, \mathbf{x}_{1}\right) &amp; \ldots &amp;k\left(\mathbf{x}_{t}, \mathbf{x}_{t}\right)\end{array}\right]\]</span></p><p>在计算出均值向量和协方差矩阵之后，可以根据此多维正态分布预测<spanclass="math inline">\(f(x)\)</span>在任意点处的概率分布。假设已经得到了一组样本<spanclass="math inline">\(X_{1:t}\)</span>，以及对应的函数值<spanclass="math inline">\(f(x_{1:t})\)</span>，如果要预测新的点<spanclass="math inline">\(x\)</span>的函数值<spanclass="math inline">\(f(x)\)</span>的期望<spanclass="math inline">\(\mu(x)\)</span>和方差<spanclass="math inline">\(\sigma^{2}(x)\)</span>，令<spanclass="math inline">\(x_{t+1}=x\)</span>，加入该点后，<spanclass="math inline">\(f(x_{1:t+1})\)</span>服从<spanclass="math inline">\(t+1\)</span>维正态分布，即：</p><p><span class="math display">\[\left[\begin{array}{c}f\left(\mathbf{x}_{1: t}\right) \\f\left(\mathbf{x}_{t+1}\right)\end{array}\right] \sim N\left(\left[\begin{array}{c}\mu\left(\mathbf{x}_{1: t}\right) \\\mu\left(\mathbf{x}_{t+1}\right)\end{array}\right],\left[\begin{array}{cc}\mathbf{K} &amp; \mathbf{k} \\\mathbf{k}^{\mathrm{T}} &amp; k\left(\mathbf{x}_{t+1},\mathbf{x}_{t+1}\right)\end{array}\right]\right)\]</span></p><p>在已知<span class="math inline">\(f(x_{1:t})\)</span>的情况下，<spanclass="math inline">\(f(x_{t+1})\)</span>服从一维正态分布，即：</p><p><span class="math display">\[f\left(\mathbf{x}_{t+1}\right) \midf\left(\mathbf{x}_{1: t}\right) \sim N\left(\mu,\sigma^{2}\right)\]</span></p><p>可以计算出对应的均值和方差，公式如下：</p><p><span class="math display">\[\begin{array}{l}\mu=\mathbf{k}^{\mathrm{T}} \mathbf{K}^{-1}\left(f\left(\mathbf{x}_{1:t}\right)-\mu\left(\mathbf{x}_{1:t}\right)\right)+\mu\left(\mathbf{x}_{t+1}\right) \\\sigma^{2}=k\left(\mathbf{x}_{t+1},\mathbf{x}_{t+1}\right)-\mathbf{k}^{\mathrm{T}} \mathbf{K}^{-1}\mathbf{k}\end{array}\]</span></p><p>计算均值利用了已有采样点处函数值<spanclass="math inline">\(f(x_{1:t})\)</span>，方差只与协方差值有关，与<spanclass="math inline">\(f(x_{1:t})\)</span>无关</p><h3 id="gpr代码实现">GPR代码实现</h3><ul><li>定义高斯核</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">kernel</span>(<span class="params">X1, X2, l=<span class="number">1.0</span>, sigma_f=<span class="number">1.0</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    X1: Array of m points (m x d).</span></span><br><span class="line"><span class="string">    X2: Array of n points (n x d).</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Covariance matrix (m x n).</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    sqdist = np.<span class="built_in">sum</span>(X1**<span class="number">2</span>, <span class="number">1</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>) + np.<span class="built_in">sum</span>(X2**<span class="number">2</span>, <span class="number">1</span>) - <span class="number">2</span> * np.dot(X1, X2.T)</span><br><span class="line">    <span class="keyword">return</span> sigma_f**<span class="number">2</span> * np.exp(-<span class="number">0.5</span> / l**<span class="number">2</span> * sqdist)</span><br></pre></td></tr></table></figure><ul><li>计算均值和协方差矩阵</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">posterior_predictive</span>(<span class="params">X_s, X_train, Y_train, l=<span class="number">1.0</span>, sigma_f=<span class="number">1.0</span>, sigma_y=<span class="number">1e-8</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    根据先验数据点计算均值向量和协方差矩阵</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X_s: New input locations (n x d).</span></span><br><span class="line"><span class="string">        X_train: Training locations (m x d).</span></span><br><span class="line"><span class="string">        Y_train: Training targets (m x 1).</span></span><br><span class="line"><span class="string">        l: Kernel length parameter.</span></span><br><span class="line"><span class="string">        sigma_f: Kernel vertical variation parameter.</span></span><br><span class="line"><span class="string">        sigma_y: Noise parameter.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Posterior mean vector (n x d) and covariance matrix (n x n).</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    K = kernel(X_train, X_train, l, sigma_f) + sigma_y**<span class="number">2</span> * np.eye(<span class="built_in">len</span>(X_train))</span><br><span class="line">    K_s = kernel(X_train, X_s, l, sigma_f)</span><br><span class="line">    K_ss = kernel(X_s, X_s, l, sigma_f) + <span class="number">1e-8</span> * np.eye(<span class="built_in">len</span>(X_s))</span><br><span class="line">    K_inv = np.linalg.inv(K)</span><br><span class="line">    mu_s = K_s.T.dot(K_inv).dot(Y_train)      <span class="comment"># 均值向量, 注意均值函数被设置为 0</span></span><br><span class="line">    cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)  <span class="comment"># 协方差矩阵</span></span><br><span class="line">    <span class="keyword">return</span> mu_s, cov_s</span><br></pre></td></tr></table></figure><ul><li>GPR拟合效果绘制</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_gp</span>(<span class="params">mu, cov, X, X_train=<span class="literal">None</span>, Y_train=<span class="literal">None</span>, samples=[]</span>):</span><br><span class="line">    <span class="comment"># 定义gp绘图函数</span></span><br><span class="line">    X = X.ravel()</span><br><span class="line">    mu = mu.ravel()</span><br><span class="line">    uncertainty = <span class="number">1.96</span> * np.sqrt(np.diag(cov))   <span class="comment"># 1.96倍标准差对应95%置信度区间</span></span><br><span class="line"></span><br><span class="line">    plt.fill_between(X, mu + uncertainty, mu - uncertainty, alpha=<span class="number">0.1</span>)</span><br><span class="line">    plt.plot(X, mu, label=<span class="string">&#x27;Mean&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(samples):</span><br><span class="line">      plt.plot(X, sample, lw=<span class="number">1</span>, ls=<span class="string">&#x27;--&#x27;</span>, label=<span class="string">f&#x27;Sample <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>&#x27;</span>)</span><br><span class="line">      <span class="keyword">if</span> X_train <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.plot(X_train, Y_train, <span class="string">&#x27;rx&#x27;</span>)</span><br><span class="line">        plt.legend()</span><br><span class="line"></span><br><span class="line">X = np.arange(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">0.2</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">X_train = np.array([-<span class="number">4</span>, -<span class="number">3</span>, -<span class="number">2</span>, -<span class="number">1</span>, <span class="number">1</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">Y_train = np.sin(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均值向量和协方差矩阵</span></span><br><span class="line">mu_s, cov_s = posterior_predictive(X, X_train, Y_train)</span><br><span class="line"></span><br><span class="line">samples = np.random.multivariate_normal(mu_s.ravel(), cov_s, <span class="number">1</span>)</span><br><span class="line">plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train, samples=samples)</span><br></pre></td></tr></table></figure><p>结果如下图所示：</p><p><img src="https://images.bumpchicken.cn/img/20220510004441.png" width="50%" height="50%"></p><p>其中，红色叉代表观测值，蓝色线代表均值，浅蓝色区域代表95%置信区间。</p><h2 id="贝叶斯优化原理">贝叶斯优化原理</h2><h4 id="基本过程">基本过程</h4><p>以下是贝叶斯优化的过程</p><p><img src="https://images.bumpchicken.cn/img/20220510004715.png"></p><p>有两个主要组成部分：</p><ol type="1"><li>GPR。根据观测点构建高斯过程回归模型，该模型能求取任意点处的函数值及后验概率。</li><li>构造采集函数（acquisitionfunction），用于决定本次迭代在哪个点处进行采样。</li></ol><p>算法首先初始化<spanclass="math inline">\(n_{0}\)</span>个点，设定最大迭代次数<spanclass="math inline">\(N\)</span>，开始循环求解，每次增加一个点，寻找下一个点时根据已经找到的<spanclass="math inline">\(n\)</span>个候选解建立高斯回归模型，通过这个模型能得到任意点处的函数值的后验概率。然后根据后验概率构造采集函数，寻找采集函数的极大值点作为下一个搜索点，以此循环，直到达到最大迭代次数，返回<spanclass="math inline">\(N\)</span>个解中的极大值作为最优解。</p><p>采集函数的选择有很多种，最常用的是期望改进（ExpectedImprovement，EI),下一节介绍下EI的原理。</p><h4 id="采集函数">采集函数</h4><p>假设已经搜索了n个点，这些点中的函数极大值记为</p><p><span class="math display">\[f_{n}^{*} =max(f(x_{1},...,f(x_{n}))\]</span></p><p>考虑下一个搜索点，计算该点处的函数值<spanclass="math inline">\(f(x)\)</span>，如果<spanclass="math inline">\(f(x)&gt;=f_{n}^{*}\)</span>，则这<spanclass="math inline">\(n+1\)</span>个点处的函数极大值为<spanclass="math inline">\(f(x)\)</span>，否则为<spanclass="math inline">\(f_{n}^{*}\)</span></p><p>加入这个新的点后，函数值的改进可以记为</p><p><span class="math display">\[e^{+} = max(0, f(x) -f_{n}^{*})\]</span></p><p>我们的目标是找到使得上面的改进值最大的<spanclass="math inline">\(x\)</span>，但是该点的函数值在我们找到<spanclass="math inline">\(x\)</span>是多少前又是未知的，幸运的是我们知道<spanclass="math inline">\(f(x)\)</span>的概率分布，因此我们可以计算在所有x处的改进值的数学期望，然后选择期望最大的<spanclass="math inline">\(x\)</span>作为下一个搜索点。定义期望改进（EI）函数如下：</p><p><span class="math display">\[EI_{n}(x) = E_{n}[max(0, f(x) -f_{n}^{*})]\]</span></p><p>令<span class="math inline">\(z=f(x)\)</span>，则有：</p><p><span class="math display">\[\begin{aligned}\mathrm{EI}_{n}(\mathbf{x}) &amp;=\int_{-\infty}^{+\infty}\left (max(0,z-f_{n}^{*})\right) \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(-\frac{(z-\mu)^{2}}{2 \sigma^{2}}\right) d z \\&amp;=\int_{f_{n}^{*}}^{+\infty}\left(z-f_{n}^{*}\right)\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(z-\mu)^{2}}{2\sigma^{2}}\right) d z\end{aligned}\]</span></p><p>换元法，得到：</p><p><span class="math display">\[\\\begin{array}{c}\int_{f_{n}^{*}}^{+\infty}\left(z-f_{n}^{*}\right) \frac{1}{\sqrt{2 \pi}\sigma} \exp \left(-\frac{(z-\mu)^{2}}{2 \sigma^{2}}\right) d z\\=\left(\mu-f_{n}^{*}\right)\left(1-\Phi\left(\left(f_{n}^{*}-\mu\right)/ \sigma\right)\right)+\sigma \varphi\left(\left(f_{n}^{*}-\mu\right) /\sigma\right)\end{array}\]</span></p><p>其中，<spanclass="math inline">\(\varphi(x)\)</span>是标准正态分布的概率密度函数，<spanclass="math inline">\(\phi(x)\)</span>是是标准正态分布的分布函数</p><p>我们的目标是求EI的极值获取下一个采样点，即</p><p><span class="math display">\[x_{n+1} = argmax EI_{n}(x)\]</span></p><p>现在目标函数已知，且能得到目标函数的一阶导数和二阶导数，可以通过梯度下降法或L-BFGS求解极值，这里不再展开叙述</p><h2 id="贝叶斯优化应用">贝叶斯优化应用</h2><p>BO有许多开源的实现，scikit-optimize 以及 参考资料4 <ahref="https://github.com/fmfn/BayesianOptimization">BayesianOptimization</a>都封装了BO，这里我们采用参考资料4中封装的BO进行演示。</p><ol type="1"><li>直接用pip安装BayesianOptimization</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install bayesian-optimization</span><br></pre></td></tr></table></figure><ol start="2" type="1"><li>定义黑盒函数</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">black_box_function</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    x,y 均是待调优参数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> -x ** <span class="number">2</span> - (y - <span class="number">1</span>) ** <span class="number">2</span> + <span class="number">1</span>   </span><br></pre></td></tr></table></figure><ol start="3" type="1"><li>初始化BO</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br><span class="line"></span><br><span class="line">pbounds = &#123;<span class="string">&#x27;x&#x27;</span>: (<span class="number">2</span>, <span class="number">4</span>), <span class="string">&#x27;y&#x27;</span>: (-<span class="number">3</span>, <span class="number">3</span>)&#125;      <span class="comment"># 设定x, y 调参范围</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化bo</span></span><br><span class="line">optimizer = BayesianOptimization(</span><br><span class="line">    f=black_box_function,</span><br><span class="line">    pbounds=pbounds,</span><br><span class="line">    random_state=<span class="number">1</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ol start="4" type="1"><li>进行迭代</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer.maximize(</span><br><span class="line">    init_points=<span class="number">2</span>,          <span class="comment"># 初始解个数</span></span><br><span class="line">    n_iter=<span class="number">20</span>,               <span class="comment"># 迭代次数</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(optimizer.<span class="built_in">max</span>)       <span class="comment"># 输出最大值及对应的参数组合</span></span><br></pre></td></tr></table></figure><p>输出：</p><p><img src="https://images.bumpchicken.cn/img/20220511172849.png" width="80%" height="80%"></p><p>函数<span class="math inline">\(f(x,y) = -x^{2} - (y-1)^{2} +1\)</span>，当<span class="math inline">\(x\in[2,4]\)</span>，<spanclass="math inline">\(y\in[-3,3]\)</span>时，很显然，当<spanclass="math inline">\(x=2,y=1\)</span>时能取到最大值，BO给出的解已经相当接近最优解</p><p>运行了多次，BO给出的解非常稳健，如下所示：</p><p><img src="https://images.bumpchicken.cn/img/20220511173250.png"></p><h2 id="参考资料">参考资料</h2><ol type="1"><li><p>《机器学习 原理、算法与应用》 雷明著</p></li><li><p>Frazier P I. A tutorial on bayesian optimization[J]. arXivpreprint arXiv:1807.02811, 2018.</p></li><li><p>https://github.com/krasserm/bayesian-machine-learning</p></li><li><p>https://github.com/fmfn/BayesianOptimization</p></li><li><p>https://github.com/scikit-optimize/scikit-optimize</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://images.bumpchicken.cn/img/20220509122455.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;贝叶斯优化(Bayesian Optimization,
BO)是一种黑盒优化算法，用于求解表达式未知的函数极值问题。算法使用高斯过程回归对一组采样点的函数值进行概率建模，预测出任意点处函数值的概率分布，然后构造采集函数(Acquistion
Function)，用于衡量每一个点值得探索(explore)的程度，求解采集函数的极值从而确定下一个采样点，最后返回这组采样点的极值作为函数的极值。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.bumpchicken.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="AutoML" scheme="http://www.bumpchicken.cn/tags/AutoML/"/>
    
  </entry>
  
  <entry>
    <title>PageRank算法原理</title>
    <link href="http://www.bumpchicken.cn/2020/12/08/PageRank/"/>
    <id>http://www.bumpchicken.cn/2020/12/08/PageRank/</id>
    <published>2020-12-08T10:05:00.000Z</published>
    <updated>2022-05-08T16:55:06.028Z</updated>
    
    <content type="html"><![CDATA[<p>PageRank,网页排名，又称网页级别，佩奇排名等，是一种由搜索引擎根据网页之间相互的超链接计算的技术，作为网页排名的要素之一，以Google创办人拉里佩奇(LarryPage)命名。Google用它来体现网页的相关性和重要性，在搜索引擎优化操作中是经常被用来评估网页优化的成效因素之一。</p><span id="more"></span><h2 id="pagerank简单形式">PageRank简单形式</h2><h3 id="基本思想">基本思想</h3><ul><li>如果一个网页被其他很多网页链接，则重要，权值高</li><li>如果PageRank值高的网页链接某个网页，则该网页权值也会相应提高</li></ul><h3 id="计算方式">计算方式</h3><p>假设有如下四个网页A、B、C、D，链接信息如下图所示:</p><p><img src="https://images.bumpchicken.cn/img/20220508234438.png" width="50%" height="50%"></p><p>上图是一个有向图，将网页看成节点，网页之间的链接关系用边表示，出链指的是链接出去的链接，入链指的是进来的链接，比如上图A有2个入链，3个出链</p><p><strong>PageRank定义</strong>: 一个网页的影响力 =所有入链集合的页面加权影响力之和</p><p>上图A节点的影响力可用如下公式计算:</p><p><span class="math display">\[PR(A) = \frac{PR(B)}{L(B)} +\frac{PR(C)}{L(C)} + \frac{PR(D)}{L(D)}\]</span></p><p>其中，<spanclass="math inline">\(PR(A)\)</span>表示网页A的影响力，<spanclass="math inline">\(L(B)\)</span>表示B的出链数量，用通用的公式表示为：</p><p><span class="math display">\[PR(u)=\sum_{\nu \in B_{u}} \frac{PR(v)}{L(v)}\]</span></p><p>u为待评估的页面，<spanclass="math inline">\(B_{u}\)</span>为页面u的入链集合。针对入链集合中的任意页面v，它能给u带来的影响力是其自身的影响力<spanclass="math inline">\(PR(v)\)</span>除以v页面的出链数量，即页面v把影响力<spanclass="math inline">\(PR(v\)</span>平均分配给了它的出链，这样统计所有能给u带来链接的页面v，得到的总和就是网页u的影响力，即为<spanclass="math inline">\(PR(u)\)</span></p><p>因此，PageRank的简单形式定义如下：</p><blockquote><p>当含有若干个节点的有向图是强连通且非周期性的有向图时，在其基础上定义的随机游走模型，即一阶马尔科夫链具有平稳分布，平稳分布向量称为这个有向图的PageRank。若矩阵M是马尔科夫链的转移矩阵，则向量R满足:<span class="math display">\[ MR = R \]</span></p></blockquote><p>上图A、B、C、D四个网页的转移矩阵M如下:</p><p><span class="math display">\[M=\left[\begin{array}{cccc}0 &amp; 1 / 2 &amp; 1 &amp; 0 \\1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\1 / 3 &amp; 1 / 2 &amp; 0 &amp; 0\end{array}\right]\]</span></p><p>假设A、B、C、D四个页面的初始影响力是相同的，即<spanclass="math inline">\(w_{0}^{T} =[1/4\space1/4\space1/4\space1/4]\)</span></p><p>第一次转移后，各页面影响力<spanclass="math inline">\(w_{1}\)</span>变为:</p><p><span class="math display">\[w_{1}=M w_{0}=\left[\begin{array}{cccc}0 &amp; 1 / 2 &amp; 1 &amp; 0 \\1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\1 / 3 &amp; 1 / 2 &amp; 0 &amp; 0\end{array}\right]\left[\begin{array}{c}1 / 4 \\1 / 4 \\1 / 4 \\1 / 4\end{array}\right]=\left[\begin{array}{l}9 / 24 \\5 / 24 \\5 / 24 \\5 / 24\end{array}\right]\]</span></p><p>之后再用转移矩阵乘以<spanclass="math inline">\(w_{1}\)</span>得到<spanclass="math inline">\(w_{2}\)</span>，直到第n次迭代后<spanclass="math inline">\(w_{n}\)</span>收敛不再变化，上述例子，<spanclass="math inline">\(w\)</span>会收敛至[0.3333 0.2222 0.22220.2222]，对应A、B、C、D的影响力</p><h3 id="等级泄露和等级沉没">等级泄露和等级沉没</h3><ol type="1"><li>等级泄露（Rank Leak):如果一个网页没有出链，就像是一个黑洞一样，吸收了其他网页的影响力而不释放，最终会导致其他网页的PR值为0，如下图所示:</li></ol><p><img src="https://images.bumpchicken.cn/img/20220509000856.png" width="50%" height="50%"></p><ol start="2" type="1"><li>等级沉没（Rank Sink):如果一个网页只有出链没有入链，计算过程迭代下来，会导致这个网页的PR值为0，入下图所示:</li></ol><p><img src="https://images.bumpchicken.cn/img/20220509001111.png" width="50%" height="50%"></p><h2 id="pagerank改进版">PageRank改进版</h2><p>为了解决简化模型中存在的等级泄露和等级沉没问题，拉里佩奇提出了PageRank的随机浏览模型。他假设了这样一个场景：</p><blockquote><p>用户并不都是按照跳转链接的方式来上网，还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意页面，比如用户就是要直接输入网址访问其他页面，虽然这个概率比较小</p></blockquote><p>所以他定义了阻尼因子d，这个因子代表了用户按照跳转链接来上网的概率，通常可以取一个固定值0.85，而<spanclass="math inline">\(1-d=0.15\)</span>则代表了用户不是通过跳转链接的方式来访问网页的概率</p><p>下式是PageRank计算影响力的改进公式:</p><p><span class="math display">\[PR(u)=\frac{1-d}{N}+d \sum_{\nu=B_{u}}\frac{P R(v)}{L(v)}\]</span></p><p>其中，N为网页总数，这样我们有可以重新迭代网页的权重计算了，因为加入了阻尼因子d，一定程度上解决了等级泄露和等级沉没的问题</p><p>同样地，定义概率转移矩阵M，则其一般公式如下:</p><p><span class="math display">\[R=d M R+\frac{1-d}{n}1\]</span></p><p>其中，<spanclass="math inline">\(d(0&lt;=d&lt;=1)\)</span>为阻尼因子，<spanclass="math inline">\(1\)</span>是所有分量为1的n维向量</p><h2 id="pagerank-代码实现">PageRank 代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csc_matrix</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pageRank</span>(<span class="params">G, s=<span class="number">.85</span>, maxerr=<span class="number">.0001</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes the pagerank for each of the n states</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    G: matrix representing state transitions</span></span><br><span class="line"><span class="string">       Gij is a binary value representing a transition from state i to j.</span></span><br><span class="line"><span class="string">    s: probability of following a transition. 1-s probability of teleporting</span></span><br><span class="line"><span class="string">       to another state.</span></span><br><span class="line"><span class="string">    maxerr: if the sum of pageranks between iterations is bellow this we will</span></span><br><span class="line"><span class="string">            have converged.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n = G.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># transform G into markov matrix A</span></span><br><span class="line">    A = csc_matrix(G, dtype=np.<span class="built_in">float</span>)</span><br><span class="line">    rsums = np.array(A.<span class="built_in">sum</span>(<span class="number">1</span>))[:, <span class="number">0</span>]</span><br><span class="line">    ri, ci = A.nonzero()</span><br><span class="line">    A.data /= rsums[ri]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># bool array of sink states</span></span><br><span class="line">    sink = rsums == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute pagerank r until we converge</span></span><br><span class="line">    ro, r = np.zeros(n), np.ones(n)</span><br><span class="line">    <span class="keyword">while</span> np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(r - ro)) &gt; maxerr:       <span class="comment"># 迭代直至收敛</span></span><br><span class="line">        ro = r.copy()</span><br><span class="line">        <span class="comment"># calculate each pagerank at a time</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n):</span><br><span class="line">            <span class="comment"># inlinks of state i</span></span><br><span class="line">            Ai = np.array(A[:, i].todense())[:, <span class="number">0</span>]</span><br><span class="line">            <span class="comment"># account for sink states</span></span><br><span class="line">            Di = sink / <span class="built_in">float</span>(n)</span><br><span class="line">            <span class="comment"># account for teleportation to state i</span></span><br><span class="line">            Ei = np.ones(n) / <span class="built_in">float</span>(n)</span><br><span class="line">            r[i] = ro.dot(Ai * s + Di * s + Ei * (<span class="number">1</span> - s))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># return normalized pagerank</span></span><br><span class="line">    <span class="keyword">return</span> r / <span class="built_in">float</span>(<span class="built_in">sum</span>(r))</span><br></pre></td></tr></table></figure><p>使用示例： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">G = np.array([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(pageRank(G,s=<span class="number">.86</span>))</span><br><span class="line">--------------------</span><br><span class="line">[<span class="number">0.12727557</span> <span class="number">0.03616954</span> <span class="number">0.12221594</span> <span class="number">0.22608452</span> <span class="number">0.28934412</span> <span class="number">0.03616954</span> <span class="number">0.16274076</span>]</span><br></pre></td></tr></table></figure></p><h2 id="参考资料">参考资料</h2><p>1.https://www.cnblogs.com/jpcflyer/p/11180263.html</p><p>2.<ahref="https://github.com/fengdu78/lihang-code/blob/master/%E7%AC%AC21%E7%AB%A0%20PageRank%E7%AE%97%E6%B3%95/21.PageRank.ipynb">PageRanknotebook</a></p><h2 id="其他">其他</h2><p>一些相似算法或改进的算法</p><p>1.LeaderRank</p><p>2.Hilltop算法</p><p>3.ExpertRank</p><p>4.HITS</p><p>5.TrustRank</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;PageRank,
网页排名，又称网页级别，佩奇排名等，是一种由搜索引擎根据网页之间相互的超链接计算的技术，作为网页排名的要素之一，以Google创办人拉里佩奇(Larry
Page)命名。Google用它来体现网页的相关性和重要性，在搜索引擎优化操作中是经常被用来评估网页优化的成效因素之一。&lt;/p&gt;</summary>
    
    
    
    <category term="根因定位" scheme="http://www.bumpchicken.cn/categories/%E6%A0%B9%E5%9B%A0%E5%AE%9A%E4%BD%8D/"/>
    
    
    <category term="排序" scheme="http://www.bumpchicken.cn/tags/%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>孤立森林原理详解</title>
    <link href="http://www.bumpchicken.cn/2020/05/25/IsolationTree/"/>
    <id>http://www.bumpchicken.cn/2020/05/25/IsolationTree/</id>
    <published>2020-05-25T03:33:00.000Z</published>
    <updated>2022-05-08T15:39:32.109Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://images.bumpchicken.cn/img/tree.png" /></p><p>孤立森林（IsolationForest）是周志华团队于2008年提出的一种具有线性复杂度的异常检测算法，被工业界广泛应用于诸如异常流量检测,金融欺诈行为检测等场景。</p><span id="more"></span><h2 id="算法原理">算法原理</h2><p>异常检测领域，通常是正常的样本占大多数，离群点占绝少数，因此大多数异常检测算法的基本思想都是对正常点构建模型，然后根据规则识别出不属于正常点模型的离群点，比较典型的算法有OneClass SVM(OCSVM), Local OutlierFactor(LOF)。和多数异常检测算法不同，孤立森林采用了一种较为高效的异常发现算法，其思路很朴素，但也足够直观有效。</p><p>考虑以下场景，一个二维平面上零零散散分布着一些点，随机使用分割线对其进行分割，直至所有但点都不可再划分（即被孤立了）。直观上来讲，可以发现那些密度很高的簇需要被切割很多次才会停止切割，但是密度很低的点很快就会停止切割到某个子空间了。</p><p><img src="https://images.bumpchicken.cn/img/20220424235501.png" width="90%" height="50%"></p><p>孤立森林分<b>训练</b>和<b>异常评估</b>两部分:</p><ul><li><b>训练:</b> 根据样本抽样构建多棵iTree，形成孤立森林</li><li><b>异常评估:</b>根据训练过程构建的孤立森林，计算待评估值的异常得分</li></ul><h2 id="训练">训练</h2><ol type="1"><li>给定训练数据集<spanclass="math inline">\(X\)</span>，确定需要构建的孤立树（iTree）个数t，按<spanclass="math inline">\(\phi\)</span>采样大小随机取样作为子样本集<spanclass="math inline">\(X^{&#39;}\)</span></li><li>在子样本集<spanclass="math inline">\(X^{&#39;}\)</span>上构建一棵孤立树(iTree)，过程如下图所示：</li></ol><p><img src="https://images.bumpchicken.cn/img/20220508001729.png" width="80%" height="20%"></p><ol type="a"><li><p>在<spanclass="math inline">\(X\)</span>中随机选择一个属性（维度），在当前样本数据范围内，随机产生一个分割点<spanclass="math inline">\(p\)</span>(介于当前维度最大和最小值之间)</p></li><li><p>此切割点即是一个超平面，将当前节点数据空间切分成2个子空间：将当前所选维度下小于p点的放在当前节点左分支，把大于p点的放在当前节点的右分支</p></li><li><p>在节点的左分支和右分支递归执行步骤a，b，不断构造新的叶子节点，直到叶子节点上只有一个数据或者树已经生长到了限制的高度</p></li><li><p>单棵iTree构建完成</p></li></ol><ol start="3" type="1"><li>按2的过程，依次构建t棵iTree，得到孤立森林</li></ol><p><img src="https://images.bumpchicken.cn/img/20220508003153.png" width="80%" height="30%"></p><h2 id="异常评估">异常评估</h2><p>构建了孤立森林(IForest)后，可以评估某个点<spanclass="math inline">\(x\)</span>的异常得分，用到如下公式:</p><p><span class="math display">\[s(x,n)=2^{-\frac{E(h(x))}{c(n}}\]</span></p><p>其中，<span class="math inline">\(h(x)\)</span> 表示<spanclass="math inline">\(x\)</span>在某棵孤立树中的路径长度，<spanclass="math inline">\(E(h(x))\)</span>表示在所有孤立树中的期望路径长度。<spanclass="math inline">\(c(n)\)</span>为样本数为n时的二叉排序树(BST)的平均搜索路径长度，用来对样本<spanclass="math inline">\(x\)</span>的期望路径长度做归一化处理。<spanclass="math inline">\(c(n)\)</span>公式如下:</p><p><span class="math display">\[c(n)=2H(n-1)-(2(n-1)/n)\]</span></p><p>其中，<span class="math inline">\(H(i)\)</span>是一个调和数，约等于<span class="math inline">\(ln(i) + \gamma\)</span>，<spanclass="math inline">\(\gamma\)</span>为欧拉常数，约等于0.5772156649</p><p>论文对于异常得分分布有如下结论：</p><ol type="1"><li><p>如果异常得分接近1，那么一定是异常点</p></li><li><p>如果异常得分远小于0.5, 那么一定不是异常点</p></li><li><p>如果样本点的异常得分均在0.5左右，那么样本中可能不存在异常点</p></li></ol><p>异常得分<span class="math inline">\(s\)</span>和<spanclass="math inline">\(E(h(x))\)</span>关系图如下所示</p><p><img src="https://images.bumpchicken.cn/img/20220508010609.png" width="50%" height="30%"></p><p>异常得分的等高线图如下所示，通常潜在的异常点<spanclass="math inline">\(s&gt;=0.6\)</span><img src="http://images.bumpchicken.cn/img/20220508010909.png" width="50%" height="30%"></p><h2 id="参考资料">参考资料</h2><p>1.Liu F T, Ting K M, Zhou Z H. Isolation forest[C]//2008 Eighth IEEEInternational Conference on Data Mining. IEEE, 2008: 413-422.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://images.bumpchicken.cn/img/tree.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;孤立森林（Isolation
Forest）是周志华团队于2008年提出的一种具有线性复杂度的异常检测算法，被工业界广泛应用于诸如异常流量检测,金融欺诈行为检测等场景。&lt;/p&gt;</summary>
    
    
    
    <category term="异常检测" scheme="http://www.bumpchicken.cn/categories/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
    
    <category term="异常检测" scheme="http://www.bumpchicken.cn/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出SVM</title>
    <link href="http://www.bumpchicken.cn/2019/09/26/SVM/"/>
    <id>http://www.bumpchicken.cn/2019/09/26/SVM/</id>
    <published>2019-09-25T16:09:00.000Z</published>
    <updated>2022-05-11T14:30:08.178Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://images.bumpchicken.cn/img/20220511220448.png" width="60%">Support Vector Machine（支持向量机），是一种非常经典的机器学习分类方法。<span id="more"></span></p><h2 id="svm基本原理">SVM基本原理</h2><p>Support VectorMachine（支持向量机），是一种非常经典的机器学习分类方法。它有严格的数学理论支持，可解释性强，不依靠统计方法，并且利用核函数技巧能有效解决一些线性不可分的场景。</p><p>给定样本集：<spanclass="math inline">\(D=\left\{\left(\boldsymbol{x}_{1},y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right),\ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}, y_{i}\in\{-1,+1\}\)</span>，如下所示，假设左上方的点为正样本，右下方的点为负样本</p><p><img src="https://images.bumpchicken.cn/img/20220511220958.png" width="60%"></p><p>寻找一个最优的分类面，使得依赖这个分类面产生的分类结果最具鲁棒性，体现在图上就是分类样本离这个分类平面尽可能远，具有最大的间隔。将这个分类的平面称为<strong>最大间隔超平面</strong>，离这个最大间隔超平面最近的点称之为<strong>支持向量（SupportVector)，分类超平面的构建只与这些少数的点有关，这也是为什么它叫作支持向量机的原因。</strong></p><p><img src="https://images.bumpchicken.cn/img/20220511221109.png" width="60%"></p><h2 id="svm最优化问题">SVM最优化问题</h2><p>SVM目的是找到各类样本点到超平面的距离最远，也就是找到最大间隔超平面。任意超平面可以用下面这个线性方程来描述：</p><p><span class="math display">\[\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}+b=0\]</span></p><p>我们知道，二维空间点(x, y) 到直线 Ax+By+C=0的距离公式为：</p><p><span class="math display">\[\frac{|A x+By+C|}{\sqrt{A^{2}+B^{2}}}\]</span></p><p>扩展到n维空间，点<span class="math inline">\(x=\left(x_{1}, x_{2}\dots x_{n}\right)\)</span>到<span class="math inline">\(w^{T}x+b=0\)</span>距离为：</p><p><span class="math display">\[\frac{\left|w^{T}x+b\right|}{\|w\|}\]</span></p><p>其中，<span class="math inline">\(\|w\|=\sqrt{w_{1}^{2}+\ldotsw_{n}^{2}}\)</span></p><p>假定分割超平面能将样本点 <strong>准确</strong>分为两类，设支持向量到最大间隔超平面的距离为d，则其他向量到最大间隔超平面的距离大于d。于是有:</p><p><span class="math display">\[\left\{\begin{array}{l}{\frac{w^{T}x+b}{\|w\|} \geq d \quad y_{i}=1} \\ {\frac{w^{T} x+b}{\|w\|} \leq-d\quad y_{i}=-1}\end{array}\right.\]</span></p><p>因为<span class="math inline">\(\|w\| d\)</span>是正数，其对目标函数优化无影响，这里令其为1，因此上式不等式组可简化为:</p><p><span class="math display">\[\left\{\begin{array}{l}{w^{T} x+b \geq 1\quad y_{i}=1} \\ {w^{T} x+b \leq-1 \quady_{i}=-1}\end{array}\right.\]</span></p><p>合并两个不等式，得到：<span class="math inline">\(y_{i}\left(w^{T}x+b\right) \geq 1\)</span></p><p>这里，我们再来看关于超平面和支持向量的图解：</p><p><img src="https://images.bumpchicken.cn/img/20220511221210.png" width="60%"></p><p>支持向量到最大间隔超平面的距离为：</p><p><span class="math display">\[d=\frac{\left|w^{T}x+b\right|}{\|w\|}\]</span></p><p>最大化这个距离：</p><p><span class="math display">\[\max 2 * \frac{\left|w^{T}x+b\right|}{\|w\|}\]</span></p><p>对于确定的样本集来说，<span class="math inline">\(\left|w^{T}x+b\right|\)</span>是个常量，因此目标函数变为：<spanclass="math inline">\(\max \frac{2}{\|w\|}\)</span>，即<spanclass="math inline">\(\min \frac{1}{2}\|w\|\)</span></p><p>为了方便计算，去除根号，目标函数转化为：<spanclass="math inline">\(\min \frac{1}{2}\|w\|^{2}\)</span></p><p>因此得到SVM的优化问题：</p><p><span class="math display">\[\min \frac{1}{2}\|w\|^{2} \]</span></p><p><span class="math display">\[{s.t.}\quad y_{i}\quad\left(w^{T}x_{i}+b\right) \geq 1\]</span></p><h2 id="kkt条件">KKT条件</h2><p>上述最优化问题约束条件是不等式。如果是等式约束，可以直接用Lagrange乘数法求解，即对于下述最优化问题</p><p><span class="math display">\[\begin{array}{c}{\min f\left(x_{1},x_{2}, \ldots, x_{n}\right)} \\ {\text { s.t. } \quad h_{k}\left(x_{1},x_{2}, \ldots, x_{n}\right)=0}\end{array}\]</span></p><p>我们可以构造拉格朗日函数：<span class="math inline">\(L(x,\lambda)=f(x)+\sum_{k=1}^{l} \lambda_{k}h_{k}(x)\)</span>，然后分别对<spanclass="math inline">\(x\)</span>,<spanclass="math inline">\(\lambda\)</span>求偏导，求得可能的极值点</p><p><span class="math display">\[\left\{\begin{array}{ll}{\frac{\partialL}{\partial x_{i}}=0} &amp; {i=1,2, \ldots, n} \\ {\frac{\partialL}{\partial \lambda_{k}}=0} &amp; {k=1,2, \ldots,l}\end{array}\right.\]</span></p><p>那么对于不等式约束条件，做法是引入一个松弛变量，然后将该松弛变量也视为待优化变量。以SVM优化问题为例：</p><p><span class="math display">\[\begin{aligned} \min f(w) &amp;=\min\frac{1}{2}\|w\|^{2} \\ \text {s.t.} &amp; g_{i}(w)=1-y_{i}\left(w^{T}x_{i}+b\right) \leq 0 \end{aligned}\]</span></p><p>引入松弛变量 <spanclass="math inline">\(a_{i}^{2}\)</span>，得到新的约束条件: <spanclass="math inline">\(h_{i}\left(w,a_{i}\right)=g_{i}(w)+a_{i}^{2}=0\)</span>，将不等式约束变为等式约束，得到新的拉格朗日函数：</p><p><span class="math display">\[\begin{aligned} L(w, \lambda, a)&amp;=\frac{1}{2} f(w)+\sum_{i=1}^{n} \lambda_{i} h_{i}(w) \\&amp;=\frac{1}{2} f(w)+\sum_{i=1}^{n}\lambda_{i}\left[g_{i}(w)+a_{i}^{2}\right] \quad \lambda_{i} \geq 0\end{aligned}\]</span></p><p>（<strong>注意到，这里有<spanclass="math inline">\(\lambda_{i}&gt;=0\)</span>，在拉格朗日乘数法中，没有非负的要求，关于这里为何<span class="math inline">\(\lambda_{i}&gt;=0\)</span>可以通过几何性质来证明，有兴趣的可以查阅相关资料</strong>。）</p><p>根据等式约束条件，有：</p><p><span class="math display">\[\left\{\begin{array}{c}{\frac{\partialL}{\partial w_{i}}=\frac{\partial f}{\partial w_{i}}+\sum_{i=1}^{n}\lambda_{i} \frac{\partial g_{i}}{\partial w_{i}}=0} \\ {\frac{\partialL}{\partial a_{i}}=2 \lambda_{i} a_{i}=0} \\ {\frac{\partial L}{\partial\lambda_{i}}=g_{i}(w)+a_{i}^{2}=0} \\ {\lambda_{i} \geq0}\end{array}\right.\]</span></p><p>第二个式子，<span class="math inline">\(2\lambda_{i}a_{i}=0\)</span>，有两种情况：</p><ol type="1"><li><span class="math inline">\(\lambda_{i}\)</span> 为0，<spanclass="math inline">\(a_{i}\)</span>不为0：由于<spanclass="math inline">\(\lambda_{i}\)</span>为0，这时候约束<spanclass="math inline">\(g_{i}(w)\)</span>不起作用，并且<spanclass="math inline">\(g_{i}(w)&lt;0\)</span></li><li><span class="math inline">\(\lambda_{i}\)</span>不为0，<spanclass="math inline">\(a_{i}\)</span>为0：这时<spanclass="math inline">\(g_{i}(w)\)</span>起约束作用，并且<spanclass="math inline">\(g_{i}(w)=0\)</span></li></ol><p>因此，方程组可转换为：</p><p><span class="math display">\[\left\{\begin{aligned} \frac{\partialL}{\partial w_{i}} &amp;=\frac{\partial f}{\partialw_{i}}+\sum_{j=1}^{n} \lambda_{j} \frac{\partial g_{j}}{\partialw_{i}}=0 \\ \lambda_{i} g_{i}(w) &amp;=0 \\ g_{i}(w) &amp; \leq 0 \\\lambda_{i} &amp; \geq 0 \end{aligned}\right.\]</span></p><p>以上便是不等式约束优化问题的__KKT(Karush-Kuhn-Tucker)条件__，<spanclass="math inline">\(\lambda_{i}\)</span>称为KKT乘子。从这个方程组可以得到以下讯息：</p><ol type="1"><li>对于支持向量 <span class="math inline">\(g_{i}(w)=0\)</span>，<spanclass="math inline">\(\lambda_{i}&gt;0\)</span>即可</li><li>对于非支持向量 <spanclass="math inline">\(g_{i}(w)&lt;0\)</span>，但要求 <spanclass="math inline">\(\lambda_{i}=0\)</span></li></ol><h2 id="求解svm最优化问题">求解SVM最优化问题</h2><p>利用KKT条件，我们可以求解SVM最优化问题：</p><p><span class="math display">\[\min _{w}\frac{1}{2}\|w\|^{2}\]</span></p><p><span class="math display">\[\text {s.t. } \quad g_{i}(w, b)=1-y_{i}\left(w^{T} x_{i}+b\right) \quad \leq 0, \quad i=1,2, \ldots,n\]</span></p><p><strong>Step 1</strong>: 构造拉格朗日函数</p><p><span class="math display">\[\begin{aligned} L(w, b, \lambda)=&amp;\frac{1}{2}\|w\|^{2}+\sum_{i=1}^{n} \lambda_{i}\left[1-y_{i}\left(w^{T}x_{i}+b\right)\right] \\ &amp; \text {s.t.} \quad \lambda_{i} \geq 0\end{aligned}\]</span></p><p>假设目标函数最小值为p, 即<spanclass="math inline">\(\frac{1}{2}\|w\|^{2} = p\)</span>，因为<spanclass="math inline">\(\sum_{i=1}^{n} \lambda_{i}\left[1-y_{i}\left(w^{T}x_{i}+b\right)\right] &lt;= 0\)</span>，即<spanclass="math inline">\(L(w, b, \lambda) &lt;=p\)</span>，为了找到最优的参数<spanclass="math inline">\(\lambda\)</span>使得<spanclass="math inline">\(L(w, b, \lambda)\)</span>接近p，问题转换为：<spanclass="math inline">\(\max _{\lambda} L(w, b,\lambda)\)</span>，即：</p><p><span class="math display">\[\begin{array}{c}{\min _{w} \max_{\lambda} L(w, b, \lambda)} \\ {\text { s.t. } \quad \lambda_{i} \geq0}\end{array}\]</span></p><p><strong>Step 2</strong>:利用对偶性转换求解问题：</p><p>对偶问题其实就是将：</p><p><span class="math display">\[\begin{array}{c}{\min _{w} \max_{\lambda} L(w, b, \lambda)} \\ {\text { s.t. } \quad \lambda_{i} \geq0}\end{array}\]</span></p><p>转化为：<span class="math inline">\(\begin{array}{c}{\max _{\lambda}\min _{w} L(w, b, \lambda)} \\ {\text { s.t. } \quad \lambda_{i} \geq0}\end{array}\)</span></p><p>假设有函数<span class="math inline">\(f\)</span>,我们有：</p><p><span class="math display">\[min \space max f &gt;= max \space minf\]</span></p><p>即最大的里面挑出来最小的也要比最小的里面挑出来最大的要大，这是一种<strong>弱对偶关系</strong>，而 <strong>强对偶关系</strong>是当等号成立时，即：</p><p><span class="math display">\[min \space max f == max \space minf\]</span></p><p>当<spanclass="math inline">\(f\)</span>是凸优化问题时，等号成立，而我们之前求的KKT条件是强对偶性的<strong>充要条件</strong></p><p>因此，对<span class="math inline">\(\begin{array}{c}{\max _{\lambda}\min _{w} L(w, b, \lambda)} \\ {\text { s.t. } \quad \lambda_{i} \geq0}\end{array}\)</span>进行求解：</p><p>1）对参数<span class="math inline">\(w\)</span>和<spanclass="math inline">\(b\)</span>求偏导数：</p><p><span class="math display">\[\frac{\partial L}{\partialw}=w-\sum_{i=1}^{n} \lambda_{i} x_{i} y_{i}=0\]</span></p><p><span class="math display">\[\frac{\partial L}{\partialb}=\sum_{i=1}^{n} \lambda_{i} y_{i}=0\]</span></p><p>得到：</p><p><span class="math display">\[\sum_{i=1}^{n} \lambda_{i} x_{i}y_{i}=w\]</span></p><p><span class="math display">\[\sum_{i=1}^{n} \lambda_{i}y_{i}=0\]</span></p><p>2）将1）中求导结果代回 <span class="math inline">\(L(w, b,\lambda)\)</span>中，得到：</p><p><span class="math display">\[\begin{aligned} L(w, b, \lambda)&amp;=\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_{i} \lambda_{j}y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{n}\lambda_{i}-\sum_{i=1}^{n} \lambda_{i} y_{i}\left(\sum_{j=1}^{n}\lambda_{j} y_{j}\left(x_{i} \cdot x_{j}\right)+b\right) \\&amp;=\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_{i} \lambda_{j}y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{n}\lambda_{i}-\sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_{i} \lambda_{j} y_{i}y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{n} \lambda_{i} y_{i} b\\ &amp;=\sum_{j=1}^{n} \lambda_{i}-\frac{1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n} \lambda_{i} \lambda_{j} y_{i} y_{j}\left(x_{i} \cdotx_{j}\right) \end{aligned}\]</span></p><p>3）利用SMO（Sequential Minimal Optimization）求解</p><p><span class="math inline">\(\max _{\lambda}\left[\sum_{j=1}^{n}\lambda_{i}-\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_{i}\lambda_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)\right]\)</span><span class="math inline">\(\text { s.t. } \sum_{i=1}^{n} \lambda_{i}y_{i}=0 \quad \lambda_{i} \geq 0\)</span></p><p>这是一个二次规划问题，问题规模正比于训练样本数，我们常用SMO算法求解。</p><p>SMO的思路是：先固定除<spanclass="math inline">\(\lambda_{i}\)</span>之外的参数，然后求<spanclass="math inline">\(\lambda_{i}\)</span>上的极值。但是我们这里有约束条件<spanclass="math inline">\(\sum_{i=1}^{n} \lambda_{i}y_{i}=0\)</span>，如果固定<spanclass="math inline">\(\lambda_{i}\)</span>之外的参数，<spanclass="math inline">\(\lambda_{i}\)</span>可直接由其他参数推导出来。因此这里SMO一次优化两个参数，具体步骤为：</p><ul><li><p>选择两个需要更新的参数<spanclass="math inline">\(\lambda_{i}\)</span>和<spanclass="math inline">\(\lambda_{j}\)</span>，固定其他参数，于是约束变为：</p><p><span class="math inline">\(\lambda_{i} y_{i}+\lambda_{j} y_{j}=c\quad \lambda_{i} \geq 0, \lambda_{j} \geq 0\)</span>，其中，<spanclass="math inline">\(c=-\sum_{k \neq i, j} \lambda_{k}y_{k}\)</span></p><p>得到：<span class="math inline">\(\lambda_{j}=\frac{c-\lambda_{i}y_{i}}{y_{j}}\)</span></p><p>也就是说我们可以用<spanclass="math inline">\(\lambda_{i}\)</span>的表达式代替<spanclass="math inline">\(\lambda_{j}\)</span>。这样就相当于把目标问题转化成了仅有一个约束条件的最优化问题，仅有的约束是<spanclass="math inline">\(\lambda_{i}&gt;=0\)</span></p></li><li><p>对于仅有一个约束条件的最优化问题，我们完全可以在<spanclass="math inline">\(\lambda_{i}\)</span>上对优化目标求偏导，令导数为零，从而求出变量值<spanclass="math inline">\(\lambda_{i}\)</span>的极值<spanclass="math inline">\(\lambda_{i_{new}}\)</span>，然后通过<spanclass="math inline">\(\lambda_{i_{new}}\)</span>求出<spanclass="math inline">\(\lambda_{j_{new}}\)</span></p></li><li><p>多次迭代直至收敛</p></li></ul><p>4）根据 <span class="math inline">\({\sum_{i=1}^{n} \lambda_{i} x_{i}y_{i}=w}\)</span>求得<span class="math inline">\(w\)</span></p><p>5）求偏移项<span class="math inline">\(b\)</span></p><p>对于<span class="math inline">\(\lambda_{i}&gt;0\)</span>，<spanclass="math inline">\(g_{i}(w)=0\)</span>，满足这个条件的点均为支持向量，可以取任一支持向量，带入<spanclass="math inline">\(y_{s}\left(wx_{s}+b\right)=1\)</span>，即可求得<spanclass="math inline">\(b\)</span></p><p>或者采取更为鲁棒的做法，取所有支持向量各计算出一个<spanclass="math inline">\(b\)</span>，然后取均值，即按下式求取：</p><p><span class="math display">\[b=\frac{1}{|S|} \sum_{s \inS}\left(y_{s}-w x_{s}\right)\]</span></p><p>6）构造分类超平面：<span class="math inline">\(w^{T}x+b=0\)</span></p><p>分类决策函数：<spanclass="math inline">\(f(x)=\operatorname{sign}\left(w^{T}x+b\right)\)</span></p><p>其中，<span class="math inline">\(sign(x)\)</span>是阶跃函数：</p><p><spanclass="math display">\[\operatorname{sign}(x)=\left\{\begin{array}{rl}{-1}&amp; {x&lt;0} \\ {0} &amp; {x=0} \\ {1} &amp;{x&gt;0}\end{array}\right.\]</span></p><h2 id="线性不可分场景">线性不可分场景</h2><p>以上我们讨论的都是线性可分的情况，实际场景中，通常遇到的数据分布都是线性不可分的。如以下场景，此场景下，求得的分类面损失将会超出我们的容忍范围。</p><p><img src="https://images.bumpchicken.cn/img/20220511221306.png" width="60%"></p><p><strong>SVM的做法是将二维线性不可分样本映射到高维空间中，让样本点在高维空间线性可分</strong>，比如下列动图演示的做法</p><p><img src="https://images.bumpchicken.cn/img/201485427.gif" width="80%"></p><p>对于在有限维度向量空间中线性不可分的样本，我们将其映射到更高维度的向量空间里，再通过间隔最大化的方式，学习得到的支持向量机称之为<strong>非线性SVM。</strong></p><p>我们用 x 表示原来的样本点，用 <spanclass="math inline">\(\kappa(x)\)</span>表示 <spanclass="math inline">\(x\)</span>映射到特征新的特征空间后到新向量。那么优化问题可以表示为：</p><p><span class="math display">\[\max _{\boldsymbol{\lambda}}\sum_{i=1}^{m} \lambda{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m}\lambda{i} \lambda{j} y_{i} y_{j} \kappa\left(\boldsymbol{x}_{i},\boldsymbol{x}_{j}\right)\]</span></p><p><span class="math display">\[\text {s.t.} \quad \sum_{i=1}^{n}\lambda_{i} y_{i}=0, \quad \lambda_{i} \geq 0\]</span></p><p>我们常用的核函数有：</p><ul><li><p>线性核函数</p><p><span class="math display">\[k\left(x_{i}, x_{j}\right)=x_{i}^{T}x_{j}\]</span></p></li><li><p>多项式核函数</p><p><span class="math display">\[k\left(x_{i},x_{j}\right)=\left(x_{i}^{T} x_{j}\right)^{d}\]</span></p></li><li><p>高斯核函数</p><p><span class="math display">\[k\left(x_{i}, x_{j}\right)=\exp\left(-\frac{\left\|x_{i}-x_{j}\right\|}{2\delta^{2}}\right)\]</span></p></li></ul><p>理论上高斯核函数可以将数据映射到无限维。</p><h2 id="总结">总结</h2><p>SVM作为一种经典的机器学习分类方法，具有以下优点:</p><ol type="1"><li>采用核技巧之后，可以处理非线性分类/回归任务</li><li>能找出对任务至关重要的关键样本（即支持向量)</li><li>最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”</li></ol><p>同时，也具有以下缺点：</p><ol type="1"><li>训练时间长。当采用 SMO算法时，由于每次都需要挑选一对参数，因此时间复杂度为 O(N2)，其中 N为训练样本的数量</li><li>当采用核技巧时，如果需要存储核矩阵，则空间复杂度为 O(N2)</li><li>模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高</li></ol><h2 id="参考资料">参考资料</h2><ol type="1"><li><p>《机器学习》 周志华</p></li><li><p><ahref="https://zhuanlan.zhihu.com/p/26514613">浅谈最优化问题中的KKT条件</a></p></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://images.bumpchicken.cn/img/20220511220448.png&quot; width=&quot;60%&quot;&gt;
Support Vector Machine（支持向量机），是一种非常经典的机器学习分类方法。</summary>
    
    
    
    <category term="机器学习" scheme="http://www.bumpchicken.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="分类" scheme="http://www.bumpchicken.cn/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>动态归整距离（DTW）详解</title>
    <link href="http://www.bumpchicken.cn/2019/04/08/DTW/"/>
    <id>http://www.bumpchicken.cn/2019/04/08/DTW/</id>
    <published>2019-04-08T11:51:00.000Z</published>
    <updated>2022-05-09T06:16:42.630Z</updated>
    
    <content type="html"><![CDATA[<p>DTW(Dynamic TimeWarping)动态归整距离是一种衡量两个时间序列相似度的方法</p><span id="more"></span><h2 id="dtw原理">DTW原理</h2><p>在时间序列分析中，需要比较相似性的两段时间序列的长度可能并不相等，在语音识别领域表现为不同人的语速不同。而且同一个单词内的不同音素的发音速度也不同，比如有的人会把‘A’这个音拖得很长，或者把‘i’发的很短。另外，不同时间序列可能仅仅存在时间轴上的位移，亦即在还原位移的情况下，两个时间序列是一致的。在这些复杂情况下，使用传统的欧几里得距离无法有效地求的两个时间序列之间的距离(或者相似性)。</p><p><strong>DTW通过把时间序列进行延伸和缩短，来计算两个时间序列性之间的相似性</strong></p><p><img src="https://images.bumpchicken.cn/img/20220509012950.png" width="50%" height="50%"></p><p>如上图所示，上下两条实线代表两个时间序列，它们之间的虚线代表两个时间序列之间相似的点。DTW使用所有这些相似点之间距离的和，称之为归整路径距离（WarpPath Distance)</p><h2 id="dtw计算方法">DTW计算方法</h2><p>令要计算相似度的两个时间序列分别为<spanclass="math inline">\(X\)</span>和<spanclass="math inline">\(Y\)</span>，长度分别为<spanclass="math inline">\(|X|\)</span>和<spanclass="math inline">\(|Y|\)</span></p><h3 id="归整路径warp-path">归整路径(Warp Path)</h3><p>归整路径的形式为<spanclass="math inline">\(W=w_{1},w_{2},...,w_{k}\)</span>,其中<spanclass="math inline">\(Max(|x|,|Y|)&lt;= K &lt;= |X| + |Y|\)</span> <spanclass="math inline">\(w_{k}\)</span>的形式为<spanclass="math inline">\((i,j)\)</span>，其中<spanclass="math inline">\(i\)</span>表示的是<spanclass="math inline">\(X\)</span>中的<spanclass="math inline">\(i\)</span>坐标，<spanclass="math inline">\(j\)</span>表示的是<spanclass="math inline">\(Y\)</span>中的<spanclass="math inline">\(j\)</span>坐标。 归整路径<spanclass="math inline">\(W\)</span>必须从<spanclass="math inline">\(w_{1}=(1,1)\)</span> 开始，到<spanclass="math inline">\(w_{k}=(|X|,|Y|)\)</span>结尾，以保证<spanclass="math inline">\(X\)</span>和<spanclass="math inline">\(Y\)</span>中的每个坐标都在<spanclass="math inline">\(W\)</span>中出现。 另外，<spanclass="math inline">\(w_{k}\)</span>中<spanclass="math inline">\((i,j)\)</span>必须是单调增加的，以保证上图中的虚线不会相交，所谓单调增加是指：</p><p><span class="math display">\[w_{k}=(i,j),w_{k+1}=(i^{&#39;},j^{&#39;}) \qquad i&lt;=i^{&#39;}&lt;=i+1,j&lt;=j^{&#39;}&lt;=j+1\]</span></p><p>最后得到的归整路径是距离最短的一个路径:</p><p><span class="math display">\[Dist(W)=\sum^{k=K}_{k=1}Dist(w_{ki},w_{kj})\]</span></p><p>其中<span class="math inline">\(Dist(w_{ki},w_{kj}\)</span>为任意经典的距离计算方法，比如欧氏距离。<spanclass="math inline">\(w_{ki}\)</span>是指<spanclass="math inline">\(X\)</span>的第i个数据点，<spanclass="math inline">\(w_{kj}\)</span>是指<spanclass="math inline">\(Y\)</span>的第<spanclass="math inline">\(j\)</span>个数据点</p><h3 id="dtw实现">DTW实现</h3><p>在实现DTW时，我们采用动态规划的思想，令<spanclass="math inline">\(D(i,j)\)</span>表示长度为<spanclass="math inline">\(i\)</span>和<spanclass="math inline">\(j\)</span>的两个时间序列之间的归整路径距离:</p><p><spanclass="math display">\[D(i,j)=Dist(i,j)+min[D(i-1,j),D(i,j-1),D(i-1,j-1)]\]</span></p><p><img src="https://images.bumpchicken.cn/img/20220509015951.png" width="80%" height="30%"></p><p>代码如下: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">distance</span>(<span class="params">x,y</span>):</span><br><span class="line">   <span class="string">&quot;&quot;&quot;定义你的距离函数，欧式距离，街区距离等等&quot;&quot;&quot;</span></span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">abs</span>(x-y)</span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dtw</span>(<span class="params">X,Y</span>):</span><br><span class="line">    M=[[distance(X[i],Y[j]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X))] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(Y))]</span><br><span class="line">    l1=<span class="built_in">len</span>(X)</span><br><span class="line">    l2=<span class="built_in">len</span>(Y) </span><br><span class="line">    D=[[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(l1+<span class="number">1</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(l2+<span class="number">1</span>)]</span><br><span class="line">    D[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,l1+<span class="number">1</span>):</span><br><span class="line">      D[i][<span class="number">0</span>]=sys.maxint</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,l2+<span class="number">1</span>):</span><br><span class="line">      D[<span class="number">0</span>][j]=sys.maxint</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,l1+<span class="number">1</span>):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,l2+<span class="number">1</span>):</span><br><span class="line">        D[i][j]=M[i-<span class="number">1</span>][j-<span class="number">1</span>]+<span class="built_in">min</span>(D[i-<span class="number">1</span>][j],D[i][j-<span class="number">1</span>],D[i-<span class="number">1</span>][j-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> D[l1][l2]</span><br></pre></td></tr></table></figure></p><p><strong>DTW采用动态规划实现，时间复杂度为<spanclass="math inline">\(O(N^{2})\)</span>,有一些改进的快速DTW算法，如FastDTW[1],SparseDTW, LB_Keogh, LB_Imporved等</strong></p><h2 id="参考资料">参考资料</h2><ol type="1"><li>FastDTW: Toward Accurate Dynamic Time Warping in Linear Time andSpace. Stan Salvador, Philip Chan.</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;DTW(Dynamic Time
Warping)动态归整距离是一种衡量两个时间序列相似度的方法&lt;/p&gt;</summary>
    
    
    
    <category term="时间序列分析" scheme="http://www.bumpchicken.cn/categories/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90/"/>
    
    
    <category term="时间序列分析" scheme="http://www.bumpchicken.cn/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>DBSCAN算法原理</title>
    <link href="http://www.bumpchicken.cn/2018/09/25/Dbscan/"/>
    <id>http://www.bumpchicken.cn/2018/09/25/Dbscan/</id>
    <published>2018-09-25T08:00:00.000Z</published>
    <updated>2022-05-08T16:53:34.484Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://images.bumpchicken.cn/img/20220508173927.png" width="60%" height="20%"></p><p>DBSCAN(Density-Based Spatial Clustering of Application withNoise)是一种基于密度的空间聚类算法。该算法将具有足够密度的区域划分为簇，并在具有噪声的空间数据中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。算法无需事先指定聚类中心数目，可以对大规模无规则形状的数据进行有效聚类</p><span id="more"></span><h2 id="相关定义">相关定义</h2><p>DBSCAN有自己的一套符号体系，定义了许多新概念，数学体系十分严谨</p><h3 id="密度定义">密度定义</h3><p>给定数据集<span class="math inline">\(D\)</span></p><ul><li><span class="math inline">\(\epsilon\)</span>: 邻域半径</li><li><span class="math inline">\(\epsilon\)</span>-邻域:邻域内点的集合</li></ul><p><span class="math display">\[N_{\varepsilon}(p):=\{\text{q in datasetD} \mid \operatorname{dist}(p,q)&lt;=\varepsilon\}\]</span></p><p>【注】<em>距离度量<spanclass="math inline">\(dist(p,q)\)</span>是聚类算法中一个值得探究的问题。此处的距离度量可以为欧氏距离、曼哈顿距离等多种距离度量方式，并且数据点的维度可为任意维度</em></p><ul><li>MinPts: 核心点邻域内数据点的最小数量</li></ul><p><img src="https://images.bumpchicken.cn/img/20220508215638.png" width="50%" height="80%"></p><p>如上图，当MinPts = 4, p的密度相较于q大，p称为高密度点</p><h3 id="核心点边界点和离群点定义">核心点、边界点和离群点定义</h3><p><img src="https://images.bumpchicken.cn/img/20220508220041.png" width="60%" height="60%"></p><ul><li>核心点(Core): 高密度点，其 <spanclass="math inline">\(\epsilon\)</span>-邻域数据点数量 &gt;= MinPts</li><li>边界点(Border): 低密度点，但在某个核心点的邻域内</li><li>离群点(Outlier): 既不是核心点也不是边界点</li></ul><h3 id="密度可达定义">密度可达定义</h3><ul><li>直接密度可达： 如果p是一个核心点，切q在p的<spanclass="math inline">\(\epsilon\)</span>-邻域内，那么称q直接密度可达p</li></ul><p>【注】<em>不能说p直接密度可达q，直接密度可达不具有对称性(symmetric)</em></p><p><img src="https://images.bumpchicken.cn/img/20220508221408.png" width="40%" height="40%"></p><ul><li>密度可达:如果存在一串这样的数据点: <spanclass="math inline">\(p_{1},p_{2},...,p_{n}\)</span>，其中<spanclass="math inline">\(p_{1}=q,p_{n}=p\)</span>,且<spanclass="math inline">\(p_{i+1}\)</span>直接密度可达<spanclass="math inline">\(p_{i}\)</span>，那么称p密度可达q</li></ul><p>【注】<em>不能说q密度可达p，密度可达同样不具有对称性</em></p><p><img src="https://images.bumpchicken.cn/img/20220508222514.png" width="50%" height="50%"></p><h3 id="密度连通">密度连通</h3><p>如果p和q都密度可达点o，那么称p和q密度连通，如下所示</p><p><img src="https://images.bumpchicken.cn/img/20220508224900.png" width="50%" height="50%"></p><p>【注】<em>密度连通具有对称性，可以说q和p密度连通</em></p><h2 id="聚类准则">聚类准则</h2><p>给定一个数据集D，参数<spanclass="math inline">\(\epsilon\)</span>和MinPts，那么聚类产生的子集C必须满足两个准则：</p><ol type="1"><li>Maximality(极大性)：对于任意的p、q，如果<spanclass="math inline">\(p\in C\)</span>，且q密度可达p，那么同样<spanclass="math inline">\(q\in C\)</span></li><li>Connectivity(连通性)：对于任意的p、q，p和q是密度相连的</li></ol><h2 id="聚类流程">聚类流程</h2><p>DBSCAN聚类过程如下图所示</p><p><img src="https://images.bumpchicken.cn/img/20220508225615.png" width="80%" height="80%"></p><h2 id="参数选择">参数选择</h2><h3 id="邻域大小epsilon">邻域大小<spanclass="math inline">\(\epsilon\)</span></h3><p>DBSCAN采用全局<spanclass="math inline">\(\epsilon\)</span>和MinPts值，因此每个节点的邻域大小是一致的。当数据密度和聚簇间距离分布不均匀时，若选取较小的<spanclass="math inline">\(\epsilon\)</span>，则较稀疏的聚簇中的数据点密度会小于MintPts，会被认为是边界点而不被用于所在类的进一步扩展，可能导致较稀疏的聚簇被划分为多个性质相似的小聚簇。相反,若选取较大的<spanclass="math inline">\(\epsilon\)</span>，则离得较近而密度较大的那些聚簇可能被合并为同一个聚簇，他们之间的差异将被忽略。因此这种情况下，选取合适的邻域大小是较为困难的，当维度较高时，<spanclass="math inline">\(\epsilon\)</span>的选取更加困难</p><h3 id="minpts">MinPts</h3><p>参数MinPts的选取有一个指导性原则，即 <spanclass="math inline">\(MinPts &gt;= dim+1\)</span>，这里<spanclass="math inline">\(dim\)</span>表示聚类空间的维度大小</p><h2 id="优缺点">优缺点</h2><h3 id="优点">优点</h3><ol type="1"><li>可以对任意形状的稠密数据集进行聚类（K-Means一般只适用于凸数据集）</li><li>可以在聚类时发现异常点，对数据集的异常点不敏感</li></ol><h3 id="缺点">缺点</h3><ol type="1"><li>如果样本集的密度不均匀，聚类间距相距很大时，聚类效果较差</li><li>对于参数 <span class="math inline">\(\epsilon\)</span>和MinPts敏感，不同参数组合对聚类效果影响较大</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://images.bumpchicken.cn/img/20220508173927.png&quot; width=&quot;60%&quot; height=&quot;20%&quot;&gt;&lt;/p&gt;
&lt;p&gt;DBSCAN(Density-Based Spatial Clustering of Application with
Noise)是一种基于密度的空间聚类算法。该算法将具有足够密度的区域划分为簇，并在具有噪声的空间数据中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。算法无需事先指定聚类中心数目，可以对大规模无规则形状的数据进行有效聚类&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://www.bumpchicken.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="聚类" scheme="http://www.bumpchicken.cn/tags/%E8%81%9A%E7%B1%BB/"/>
    
  </entry>
  
</feed>
