<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.bumpchicken.cn","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="贝叶斯优化(Bayesian Optimization, BO)是一种黑盒优化算法，用于求解表达式未知的函数极值问题。算法使用高斯过程回归对一组采样点的函数值进行概率建模，预测出任意点处函数值的概率分布，然后构造采集函数(Acquistion Function)，用于衡量每一个点值得探索(explore)的程度，求解采集函数的极值从而确定下一个采样点，最后返回这组采样点的极值作为函数的极值。">
<meta property="og:type" content="article">
<meta property="og:title" content="贝叶斯优化原理及应用">
<meta property="og:url" content="http://www.bumpchicken.cn/2020/12/29/BayesianOptimization/index.html">
<meta property="og:site_name" content="BOC&#39;s warehouse">
<meta property="og:description" content="贝叶斯优化(Bayesian Optimization, BO)是一种黑盒优化算法，用于求解表达式未知的函数极值问题。算法使用高斯过程回归对一组采样点的函数值进行概率建模，预测出任意点处函数值的概率分布，然后构造采集函数(Acquistion Function)，用于衡量每一个点值得探索(explore)的程度，求解采集函数的极值从而确定下一个采样点，最后返回这组采样点的极值作为函数的极值。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://images.bumpchicken.cn/img/20220509122455.png">
<meta property="og:image" content="https://images.bumpchicken.cn/img/20220510004441.png">
<meta property="og:image" content="https://images.bumpchicken.cn/img/20220510004715.png">
<meta property="og:image" content="https://images.bumpchicken.cn/img/20220511172849.png">
<meta property="og:image" content="https://images.bumpchicken.cn/img/20220511173250.png">
<meta property="article:published_time" content="2020-12-29T04:07:00.000Z">
<meta property="article:modified_time" content="2022-05-11T09:37:53.447Z">
<meta property="article:author" content="wjd">
<meta property="article:tag" content="AutoML">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images.bumpchicken.cn/img/20220509122455.png">

<link rel="canonical" href="http://www.bumpchicken.cn/2020/12/29/BayesianOptimization/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>贝叶斯优化原理及应用 | BOC's warehouse</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="BOC's warehouse" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">BOC's warehouse</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.bumpchicken.cn/2020/12/29/BayesianOptimization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wjd">
      <meta itemprop="description" content="弱者的反击">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BOC's warehouse">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          贝叶斯优化原理及应用
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-12-29 12:07:00" itemprop="dateCreated datePublished" datetime="2020-12-29T12:07:00+08:00">2020-12-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-05-11 17:37:53" itemprop="dateModified" datetime="2022-05-11T17:37:53+08:00">2022-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="https://images.bumpchicken.cn/img/20220509122455.png"></p>
<p>贝叶斯优化(Bayesian Optimization,
BO)是一种黑盒优化算法，用于求解表达式未知的函数极值问题。算法使用高斯过程回归对一组采样点的函数值进行概率建模，预测出任意点处函数值的概率分布，然后构造采集函数(Acquistion
Function)，用于衡量每一个点值得探索(explore)的程度，求解采集函数的极值从而确定下一个采样点，最后返回这组采样点的极值作为函数的极值。</p>
<span id="more"></span>
<h2 id="黑盒优化问题">黑盒优化问题</h2>
<p>训练机器学习模型过程中，会有很多模型参数之外的参数，如学习率，卷积核大小等，再比如训练xgboost时，树的最大深度、采样率等参数都会影响训练结果，这些参数我们将其称为超参数。假设一组超参数组合<span
class="math inline">\(X=x_{1},x_{2},...,x_{n}\)</span>，存在一个未知函数
<span class="math inline">\(f:x \rightarrow \mathbb
{R}\)</span>，我们需要在<span class="math inline">\(x \in
X\)</span>找到一组最佳参数组合<span
class="math inline">\(x^{*}\)</span>使得:</p>
<p><span class="math display">\[x^{*} = \underset{x\in X}{argmin} f(x)
\]</span></p>
<p>当<span
class="math inline">\(f\)</span>是凸函数并且定义域也是凸的时候，可以通过凸优化手段(梯度下降、L_BFGS等)来求解。但是超参数优化属于黑盒优化问题，<span
class="math inline">\(f\)</span>不一定是凸函数，并且是未知的，在优化过程中只能得到函数的输入和输出，不能获取目标函数的表达式和梯度信息，这里的<span
class="math inline">\(f\)</span>通常还是计算代价非常昂贵的函数，因此优化过程会比较困难，尤其是当超参数数量大的情况。常用的超参数优化方法有网格搜索(Grid
Search)，随机搜索(Random
Search)，遗传算法（粒子群优化、模拟退火等）以及本文要介绍的贝叶斯优化方法。</p>
<p>下面介绍两种最基本的超参调优方法: 网格搜索法和随机搜索法</p>
<ul>
<li><p>网格搜索法</p>
<p>网格搜索法搜索一组离散的取值情况，得到最优参数值。如果是连续型的超参数，则需要对其定义域进行网格划分，然后选取典型值计算。网格搜索法本质上是一种穷举法，对待调优参数进行全排列组合，逐一计算<span
class="math inline">\(f\)</span>，然后选取最小的<span
class="math inline">\(f\)</span>时的参数组合，如下代码所示，给定参数候选项，我们可以列出所有的参数组合
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line">tuning_params = &#123;<span class="string">&#x27;a&#x27;</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], <span class="string">&#x27;b&#x27;</span>:[<span class="number">4</span>,<span class="number">5</span>]&#125;                     <span class="comment"># 待优化参数可选项</span></span><br><span class="line"><span class="keyword">for</span> conf <span class="keyword">in</span> product(*tuning_params.values()):</span><br><span class="line">    <span class="built_in">print</span>(&#123;k:v <span class="keyword">for</span> k,v <span class="keyword">in</span> <span class="built_in">zip</span>(tuning_params.keys(), conf)&#125;)  <span class="comment"># 生成参数组合</span></span><br></pre></td></tr></table></figure> 输出: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">4</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">5</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">4</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">5</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">4</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">5</span>&#125;</span><br></pre></td></tr></table></figure>
随着待调优参数增加，生成的全排列组合数量将非常巨大，计算代价过于昂贵</p></li>
<li><p>随机搜索法
相比于网格搜索法，随机搜索的做法是将超参数随机地取某些值，设置一个最大迭代次数，比较每次迭代中不同取值算法的输出，得到最优超参数组合。而随机取值的方法也有多种不同的做法，常用的做法是采用均匀分布的随机数进行搜索，或者采用一些启发式的搜索策略（粒子群优化算法），这里不展开赘述。随机搜索并不总能找到全局最优解，但是通常认为随机搜索比网格搜索更优，其可以花费更少的计算代价得到相近的结果。</p></li>
</ul>
<p><strong>无论是网格搜索法还是随机搜索法，每一次进行迭代计算的时候，都未曾考虑已经搜索过的空间，即搜索过的空间未对下一次搜索产生任何指导作用，因此可能存在很多无效搜索。不同于网格搜索和随机搜索法，贝叶斯优化则能够通过高斯过程回归有效利用先验的搜索空间进行下一次搜索参数的选择，能大大减少迭代次数</strong></p>
<h2 id="理论准备">理论准备</h2>
<p>经典的贝叶斯优化利用高斯过程(Gaussian Process, GP)对<span
class="math inline">\(f\)</span>进行概率建模，在介绍贝叶斯优化之前，有必要了解一下高斯过程回归的相关知识</p>
<h3 id="高斯过程">高斯过程</h3>
<p>高斯过程用于对一组随着时间增长的随机向量进行建模，在任意时刻，某个向量的所有子向量均服从高斯分布。
假设有连续型随机变量序列<span
class="math inline">\(x_{1},x_{2},...,x_{T}\)</span>，如果该序列中任意数量的随机变量构成的向量<span
class="math inline">\(X_{t_{1}, ... ,t_{k}} = [x_{t_{1}} \space ...
\space
x_{t_{k}}]^{T}\)</span>均服从多维正态分布，则称次随机变量序列为高斯过程。</p>
<p>特别地，假设当前有k个随机变量<span
class="math inline">\(x_{1},...,x{k}\)</span>，它们服从k维正态分布<span
class="math inline">\(N( \mu_{k}, \sum _{k}
)\)</span>，其中均值向量<span class="math inline">\(N( \mu_{k}, \sum
_{k} )\)</span>，协方差矩阵<span class="math inline">\(\sum _{k} \in
\mathbb R^{k*k}\)</span></p>
<p>当加入一个新的随机变量<span
class="math inline">\(x_{k+1}\)</span>之后，随机向量<span
class="math inline">\(x_{1},x_{2},...,x_{k},x_{k+1}\)</span>服从k+1维正态分布<span
class="math inline">\(\mu_{k+1} \in
\mathbb{R}^{k+1}\)</span>，其中均值向量<span
class="math inline">\(\mu_{k+1} \in
\mathbb{R}^{k+1}\)</span>，协方差矩阵<span class="math inline">\(\sum
_{k+1} \in \mathbb R^{(k+1)*(k+1)}\)</span></p>
<p>由于正态分布的积分能够得到解析解，因此可以方便地得到边缘概率于条件概率。</p>
<h3 id="高斯过程回归">高斯过程回归</h3>
<p>机器学习中，算法通常是根据输入值<span
class="math inline">\(x\)</span>预测出一个最佳输出值<span
class="math inline">\(y\)</span>，用于分类或回归。某些情况下我们需要的不是预测出一个函数值，而是给出这个函数值的后验概率分布<span
class="math inline">\(p(y|x)\)</span>。对于实际应用问题，一般是给定一组样本点<span
class="math inline">\(x_{i}, \space
i=1,…,l\)</span>，基于此拟合出一个假设函数，给定输入值<span
class="math inline">\(x\)</span>，预测其标签值或者后验概率<span
class="math inline">\(p(y|x)\)</span>，高斯过程回归对应后者。</p>
<p>高斯过程回归(Gaussian Process Regression,
GPR)对表达式未知的函数的一组函数值进行概率建模，给出函数值的概率分布。嘉定给定某些点<span
class="math inline">\(x_{i}, i=
1,…,t\)</span>，以及在这些点处的函数值<span
class="math inline">\(f(x_{i})\)</span>，GPR能够根据这些点，拟合该未知函数，那么对于任意给定的<span
class="math inline">\(x\)</span>，就可以预测出<span
class="math inline">\(f(x)\)</span>，并且能够给出预测结果的置信度。</p>
<p>GPR假设黑盒函数在各个点处的函数值<span
class="math inline">\(f(x)\)</span>都是随机变量，它们构成的随机向量服从多维正态分布。假设有t个采样点<span
class="math inline">\(x_{1},…,x_{t}\)</span>，在这些点处的函数值构成向量：</p>
<p><span class="math display">\[f(x_{1:t}) = [f(x_{1} \space ... \space
f(x_{t})]\]</span></p>
<p>GPR假设此向量服从t维正态分布：</p>
<p><span class="math display">\[f(x_{1:t}) \sim N(\mu(x_{1:t}),
\sum(x_{1:t},x_{1:t}))\]</span></p>
<p>其中，<span
class="math inline">\(\mu(x_{1:t})=[\mu(x_{1}),…,\mu(x_{t})]\)</span>是高斯分布的均值向量，<span
class="math inline">\(\sum(x_{1:t},x_{1:t})\)</span>是协方差矩阵</p>
<p><span class="math display">\[\left[\begin{array}{ccc}
\operatorname{cov}\left(\mathbf{x}_{1}, \mathbf{x}_{1}\right) &amp;
\ldots &amp; \operatorname{cov}\left(\mathbf{x}_{1},
\mathbf{x}_{t}\right) \\
\cdots &amp; \ldots &amp; \ldots \\
\operatorname{cov}\left(\mathbf{x}_{t}, \mathbf{x}_{1}\right) &amp;
\ldots &amp; \operatorname{cov}\left(\mathbf{x}_{t},
\mathbf{x}_{t}\right)
\end{array}\right]=\left[\begin{array}{ccc}
k\left(\mathbf{x}_{1}, \mathbf{x}_{1}\right) &amp; \ldots &amp;
k\left(\mathbf{x}_{1}, \mathbf{x}_{t}\right) \\
\ldots &amp; \ldots &amp; \ldots \\
k\left(\mathbf{x}_{t}, \mathbf{x}_{1}\right) &amp; \ldots &amp;
k\left(\mathbf{x}_{t}, \mathbf{x}_{t}\right)
\end{array}\right]\]</span></p>
<p>问题的关键是如何根据样本值计算出正态分布的均值向量和协方差矩阵，均值向量是通过均值函数<span
class="math inline">\(\mu(x)\)</span>根据每个采样点x计算构造的，可简单令<span
class="math inline">\(\mu(x)=c\)</span>，或者将均值设置为0，因为即使均值设置为常数，由于有方差的作用，依然能够对数据进行有效建模。</p>
<p>协方差通过核函数<span
class="math inline">\(k(x,x^{&#39;})\)</span>计算得到，也称为协方差函数，协方差函数需要满足以下要求：</p>
<ol type="1">
<li>距离相近的样本点<span class="math inline">\(x\)</span>和<span
class="math inline">\(x^{&#39;}\)</span>之间有更大的正协方差值，因为相近的两个点的函数值有更强的相关性</li>
<li>保证协方差矩阵是对称半正定矩阵</li>
</ol>
<p>常用的是高斯核和Matern核，高斯核定义为：</p>
<p><span class="math display">\[k\left(\mathbf{x}_{1},
\mathbf{x}_{2}\right)=\alpha_{0} \exp \left(-\frac{1}{2
\sigma^{2}}\left\|\mathbf{x}_{1}-\mathbf{x}_{2}\right\|^{2}\right)\]</span></p>
<p>Matern核定义为:</p>
<p><span class="math display">\[k\left(\mathbf{x}_{1},
\mathbf{x}_{2}\right)=\frac{2^{1-v}}{\Gamma(v)}\left(\sqrt{2
v}\left\|\mathbf{x}_{1}-\mathbf{x}_{2}\right\|\right)^{v}
K_{v}\left(\sqrt{2
v}\left\|\mathbf{x}_{1}-\mathbf{x}_{2}\right\|\right)\]</span></p>
<p>其中<span class="math inline">\(\Gamma\)</span>是伽马函数，<span
class="math inline">\(K_{v}\)</span>是贝塞尔函数(Bessel function)，<span
class="math inline">\(v\)</span>是人工设定的正参数。用核函数计算任意两点之间的核函数值，得到核函数矩阵<span
class="math inline">\(K\)</span>作为协方差矩阵的估计值：</p>
<p><span class="math display">\[\mathbf{K}=\left[\begin{array}{ccc}
k\left(\mathbf{x}_{1}, \mathbf{x}_{1}\right) &amp; \ldots &amp;
k\left(\mathbf{x}_{1}, \mathbf{x}_{t}\right) \\
\ldots &amp; \ldots &amp; \ldots \\
k\left(\mathbf{x}_{t}, \mathbf{x}_{1}\right) &amp; \ldots &amp;
k\left(\mathbf{x}_{t}, \mathbf{x}_{t}\right)
\end{array}\right]\]</span></p>
<p>在计算出均值向量和协方差矩阵之后，可以根据此多维正态分布预测<span
class="math inline">\(f(x)\)</span>在任意点处的概率分布。假设已经得到了一组样本<span
class="math inline">\(X_{1:t}\)</span>，以及对应的函数值<span
class="math inline">\(f(x_{1:t})\)</span>，如果要预测新的点<span
class="math inline">\(x\)</span>的函数值<span
class="math inline">\(f(x)\)</span>的期望<span
class="math inline">\(\mu(x)\)</span>和方差<span
class="math inline">\(\sigma^{2}(x)\)</span>，令<span
class="math inline">\(x_{t+1}=x\)</span>，加入该点后，<span
class="math inline">\(f(x_{1:t+1})\)</span>服从<span
class="math inline">\(t+1\)</span>维正态分布，即：</p>
<p><span class="math display">\[\left[\begin{array}{c}
f\left(\mathbf{x}_{1: t}\right) \\
f\left(\mathbf{x}_{t+1}\right)
\end{array}\right] \sim N\left(\left[\begin{array}{c}
\mu\left(\mathbf{x}_{1: t}\right) \\
\mu\left(\mathbf{x}_{t+1}\right)
\end{array}\right],\left[\begin{array}{cc}
\mathbf{K} &amp; \mathbf{k} \\
\mathbf{k}^{\mathrm{T}} &amp; k\left(\mathbf{x}_{t+1},
\mathbf{x}_{t+1}\right)
\end{array}\right]\right)\]</span></p>
<p>在已知<span class="math inline">\(f(x_{1:t})\)</span>的情况下，<span
class="math inline">\(f(x_{t+1})\)</span>服从一维正态分布，即：</p>
<p><span class="math display">\[f\left(\mathbf{x}_{t+1}\right) \mid
f\left(\mathbf{x}_{1: t}\right) \sim N\left(\mu,
\sigma^{2}\right)\]</span></p>
<p>可以计算出对应的均值和方差，公式如下：</p>
<p><span class="math display">\[\begin{array}{l}
\mu=\mathbf{k}^{\mathrm{T}} \mathbf{K}^{-1}\left(f\left(\mathbf{x}_{1:
t}\right)-\mu\left(\mathbf{x}_{1:
t}\right)\right)+\mu\left(\mathbf{x}_{t+1}\right) \\
\sigma^{2}=k\left(\mathbf{x}_{t+1},
\mathbf{x}_{t+1}\right)-\mathbf{k}^{\mathrm{T}} \mathbf{K}^{-1}
\mathbf{k}
\end{array}\]</span></p>
<p>计算均值利用了已有采样点处函数值<span
class="math inline">\(f(x_{1:t})\)</span>，方差只与协方差值有关，与<span
class="math inline">\(f(x_{1:t})\)</span>无关</p>
<h3 id="gpr代码实现">GPR代码实现</h3>
<ul>
<li>定义高斯核</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">kernel</span>(<span class="params">X1, X2, l=<span class="number">1.0</span>, sigma_f=<span class="number">1.0</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    X1: Array of m points (m x d).</span></span><br><span class="line"><span class="string">    X2: Array of n points (n x d).</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Covariance matrix (m x n).</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    sqdist = np.<span class="built_in">sum</span>(X1**<span class="number">2</span>, <span class="number">1</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>) + np.<span class="built_in">sum</span>(X2**<span class="number">2</span>, <span class="number">1</span>) - <span class="number">2</span> * np.dot(X1, X2.T)</span><br><span class="line">    <span class="keyword">return</span> sigma_f**<span class="number">2</span> * np.exp(-<span class="number">0.5</span> / l**<span class="number">2</span> * sqdist)</span><br></pre></td></tr></table></figure>
<ul>
<li>计算均值和协方差矩阵</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">posterior_predictive</span>(<span class="params">X_s, X_train, Y_train, l=<span class="number">1.0</span>, sigma_f=<span class="number">1.0</span>, sigma_y=<span class="number">1e-8</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    根据先验数据点计算均值向量和协方差矩阵</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X_s: New input locations (n x d).</span></span><br><span class="line"><span class="string">        X_train: Training locations (m x d).</span></span><br><span class="line"><span class="string">        Y_train: Training targets (m x 1).</span></span><br><span class="line"><span class="string">        l: Kernel length parameter.</span></span><br><span class="line"><span class="string">        sigma_f: Kernel vertical variation parameter.</span></span><br><span class="line"><span class="string">        sigma_y: Noise parameter.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Posterior mean vector (n x d) and covariance matrix (n x n).</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    K = kernel(X_train, X_train, l, sigma_f) + sigma_y**<span class="number">2</span> * np.eye(<span class="built_in">len</span>(X_train))</span><br><span class="line">    K_s = kernel(X_train, X_s, l, sigma_f)</span><br><span class="line">    K_ss = kernel(X_s, X_s, l, sigma_f) + <span class="number">1e-8</span> * np.eye(<span class="built_in">len</span>(X_s))</span><br><span class="line">    K_inv = np.linalg.inv(K)</span><br><span class="line">    mu_s = K_s.T.dot(K_inv).dot(Y_train)      <span class="comment"># 均值向量, 注意均值函数被设置为 0</span></span><br><span class="line">    cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)  <span class="comment"># 协方差矩阵</span></span><br><span class="line">    <span class="keyword">return</span> mu_s, cov_s</span><br></pre></td></tr></table></figure>
<ul>
<li>GPR拟合效果绘制</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_gp</span>(<span class="params">mu, cov, X, X_train=<span class="literal">None</span>, Y_train=<span class="literal">None</span>, samples=[]</span>):</span><br><span class="line">    <span class="comment"># 定义gp绘图函数</span></span><br><span class="line">    X = X.ravel()</span><br><span class="line">    mu = mu.ravel()</span><br><span class="line">    uncertainty = <span class="number">1.96</span> * np.sqrt(np.diag(cov))   <span class="comment"># 1.96倍标准差对应95%置信度区间</span></span><br><span class="line"></span><br><span class="line">    plt.fill_between(X, mu + uncertainty, mu - uncertainty, alpha=<span class="number">0.1</span>)</span><br><span class="line">    plt.plot(X, mu, label=<span class="string">&#x27;Mean&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(samples):</span><br><span class="line">      plt.plot(X, sample, lw=<span class="number">1</span>, ls=<span class="string">&#x27;--&#x27;</span>, label=<span class="string">f&#x27;Sample <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>&#x27;</span>)</span><br><span class="line">      <span class="keyword">if</span> X_train <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.plot(X_train, Y_train, <span class="string">&#x27;rx&#x27;</span>)</span><br><span class="line">        plt.legend()</span><br><span class="line"></span><br><span class="line">X = np.arange(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">0.2</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">X_train = np.array([-<span class="number">4</span>, -<span class="number">3</span>, -<span class="number">2</span>, -<span class="number">1</span>, <span class="number">1</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">Y_train = np.sin(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均值向量和协方差矩阵</span></span><br><span class="line">mu_s, cov_s = posterior_predictive(X, X_train, Y_train)</span><br><span class="line"></span><br><span class="line">samples = np.random.multivariate_normal(mu_s.ravel(), cov_s, <span class="number">1</span>)</span><br><span class="line">plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train, samples=samples)</span><br></pre></td></tr></table></figure>
<p>结果如下图所示：</p>
<p><img src="https://images.bumpchicken.cn/img/20220510004441.png" width="50%" height="50%"></p>
<p>其中，红色叉代表观测值，蓝色线代表均值，浅蓝色区域代表95%置信区间。</p>
<h2 id="贝叶斯优化原理">贝叶斯优化原理</h2>
<h4 id="基本过程">基本过程</h4>
<p>以下是贝叶斯优化的过程</p>
<p><img src="https://images.bumpchicken.cn/img/20220510004715.png"></p>
<p>有两个主要组成部分：</p>
<ol type="1">
<li>GPR。根据观测点构建高斯过程回归模型，该模型能求取任意点处的函数值及后验概率。</li>
<li>构造采集函数（acquisition
function），用于决定本次迭代在哪个点处进行采样。</li>
</ol>
<p>算法首先初始化<span
class="math inline">\(n_{0}\)</span>个点，设定最大迭代次数<span
class="math inline">\(N\)</span>，开始循环求解，每次增加一个点，寻找下一个点时根据已经找到的<span
class="math inline">\(n\)</span>个候选解建立高斯回归模型，通过这个模型能得到任意点处的函数值的后验概率。然后根据后验概率构造采集函数，寻找采集函数的极大值点作为下一个搜索点，以此循环，直到达到最大迭代次数，返回<span
class="math inline">\(N\)</span>个解中的极大值作为最优解。</p>
<p>采集函数的选择有很多种，最常用的是期望改进（Expected
Improvement，EI),下一节介绍下EI的原理。</p>
<h4 id="采集函数">采集函数</h4>
<p>假设已经搜索了n个点，这些点中的函数极大值记为</p>
<p><span class="math display">\[f_{n}^{*} =
max(f(x_{1},...,f(x_{n}))\]</span></p>
<p>考虑下一个搜索点，计算该点处的函数值<span
class="math inline">\(f(x)\)</span>，如果<span
class="math inline">\(f(x)&gt;=f_{n}^{*}\)</span>，则这<span
class="math inline">\(n+1\)</span>个点处的函数极大值为<span
class="math inline">\(f(x)\)</span>，否则为<span
class="math inline">\(f_{n}^{*}\)</span></p>
<p>加入这个新的点后，函数值的改进可以记为</p>
<p><span class="math display">\[e^{+} = max(0, f(x) -
f_{n}^{*})\]</span></p>
<p>我们的目标是找到使得上面的改进值最大的<span
class="math inline">\(x\)</span>，但是该点的函数值在我们找到<span
class="math inline">\(x\)</span>是多少前又是未知的，幸运的是我们知道<span
class="math inline">\(f(x)\)</span>的概率分布，因此我们可以计算在所有x处的改进值的数学期望，然后选择期望最大的<span
class="math inline">\(x\)</span>作为下一个搜索点。定义期望改进（EI）函数如下：</p>
<p><span class="math display">\[EI_{n}(x) = E_{n}[max(0, f(x) -
f_{n}^{*})]\]</span></p>
<p>令<span class="math inline">\(z=f(x)\)</span>，则有：</p>
<p><span class="math display">\[\begin{aligned}
\mathrm{EI}_{n}(\mathbf{x}) &amp;=\int_{-\infty}^{+\infty}\left (max(0,
z-f_{n}^{*})\right) \frac{1}{\sqrt{2 \pi} \sigma} \exp
\left(-\frac{(z-\mu)^{2}}{2 \sigma^{2}}\right) d z \\
&amp;=\int_{f_{n}^{*}}^{+\infty}\left(z-f_{n}^{*}\right)
\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(z-\mu)^{2}}{2
\sigma^{2}}\right) d z
\end{aligned}\]</span></p>
<p>换元法，得到：</p>
<p><span class="math display">\[\\
\begin{array}{c}
\int_{f_{n}^{*}}^{+\infty}\left(z-f_{n}^{*}\right) \frac{1}{\sqrt{2 \pi}
\sigma} \exp \left(-\frac{(z-\mu)^{2}}{2 \sigma^{2}}\right) d z
\\
=\left(\mu-f_{n}^{*}\right)\left(1-\Phi\left(\left(f_{n}^{*}-\mu\right)
/ \sigma\right)\right)+\sigma \varphi\left(\left(f_{n}^{*}-\mu\right) /
\sigma\right)
\end{array}\]</span></p>
<p>其中，<span
class="math inline">\(\varphi(x)\)</span>是标准正态分布的概率密度函数，<span
class="math inline">\(\phi(x)\)</span>是是标准正态分布的分布函数</p>
<p>我们的目标是求EI的极值获取下一个采样点，即</p>
<p><span class="math display">\[x_{n+1} = argmax EI_{n}(x)\]</span></p>
<p>现在目标函数已知，且能得到目标函数的一阶导数和二阶导数，可以通过梯度下降法或L-BFGS求解极值，这里不再展开叙述</p>
<h2 id="贝叶斯优化应用">贝叶斯优化应用</h2>
<p>BO有许多开源的实现，scikit-optimize 以及 参考资料4 <a
target="_blank" rel="noopener" href="https://github.com/fmfn/BayesianOptimization">BayesianOptimization</a>都封装了BO，这里我们采用参考资料4中封装的BO进行演示。</p>
<ol type="1">
<li>直接用pip安装BayesianOptimization</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install bayesian-optimization</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>定义黑盒函数</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">black_box_function</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    x,y 均是待调优参数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> -x ** <span class="number">2</span> - (y - <span class="number">1</span>) ** <span class="number">2</span> + <span class="number">1</span>   </span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>初始化BO</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br><span class="line"></span><br><span class="line">pbounds = &#123;<span class="string">&#x27;x&#x27;</span>: (<span class="number">2</span>, <span class="number">4</span>), <span class="string">&#x27;y&#x27;</span>: (-<span class="number">3</span>, <span class="number">3</span>)&#125;      <span class="comment"># 设定x, y 调参范围</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化bo</span></span><br><span class="line">optimizer = BayesianOptimization(</span><br><span class="line">    f=black_box_function,</span><br><span class="line">    pbounds=pbounds,</span><br><span class="line">    random_state=<span class="number">1</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ol start="4" type="1">
<li>进行迭代</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer.maximize(</span><br><span class="line">    init_points=<span class="number">2</span>,          <span class="comment"># 初始解个数</span></span><br><span class="line">    n_iter=<span class="number">20</span>,               <span class="comment"># 迭代次数</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(optimizer.<span class="built_in">max</span>)       <span class="comment"># 输出最大值及对应的参数组合</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p><img src="https://images.bumpchicken.cn/img/20220511172849.png" width="80%" height="80%"></p>
<p>函数<span class="math inline">\(f(x,y) = -x^{2} - (y-1)^{2} +
1\)</span>，当<span class="math inline">\(x\in[2,4]\)</span>，<span
class="math inline">\(y\in[-3,3]\)</span>时，很显然，当<span
class="math inline">\(x=2,y=1\)</span>时能取到最大值，BO给出的解已经相当接近最优解</p>
<p>运行了多次，BO给出的解非常稳健，如下所示：</p>
<p><img src="https://images.bumpchicken.cn/img/20220511173250.png"></p>
<h2 id="参考资料">参考资料</h2>
<ol type="1">
<li><p>《机器学习 原理、算法与应用》 雷明著</p></li>
<li><p>Frazier P I. A tutorial on bayesian optimization[J]. arXiv
preprint arXiv:1807.02811, 2018.</p></li>
<li><p>https://github.com/krasserm/bayesian-machine-learning</p></li>
<li><p>https://github.com/fmfn/BayesianOptimization</p></li>
<li><p>https://github.com/scikit-optimize/scikit-optimize</p></li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AutoML/" rel="tag"># AutoML</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/12/08/PageRank/" rel="prev" title="PageRank算法原理">
      <i class="fa fa-chevron-left"></i> PageRank算法原理
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%BB%91%E7%9B%92%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="nav-number">1.</span> <span class="nav-text">黑盒优化问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%90%86%E8%AE%BA%E5%87%86%E5%A4%87"><span class="nav-number">2.</span> <span class="nav-text">理论准备</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B"><span class="nav-number">2.1.</span> <span class="nav-text">高斯过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B%E5%9B%9E%E5%BD%92"><span class="nav-number">2.2.</span> <span class="nav-text">高斯过程回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gpr%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.3.</span> <span class="nav-text">GPR代码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E5%8E%9F%E7%90%86"><span class="nav-number">3.</span> <span class="nav-text">贝叶斯优化原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E8%BF%87%E7%A8%8B"><span class="nav-number">3.0.1.</span> <span class="nav-text">基本过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%87%87%E9%9B%86%E5%87%BD%E6%95%B0"><span class="nav-number">3.0.2.</span> <span class="nav-text">采集函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E5%BA%94%E7%94%A8"><span class="nav-number">4.</span> <span class="nav-text">贝叶斯优化应用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">5.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">wjd</p>
  <div class="site-description" itemprop="description">弱者的反击</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">沪ICP备2022012091号-1 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wjd</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '7d18fb7cc60b381d58b2',
      clientSecret: '779d68590ffb7283ddca9f654a481a6398509c73',
      repo        : 'wjd92.github.io',
      owner       : 'wjd92',
      admin       : ['wjd92'],
      id          : 'ab7235ec60189133059f7a23c3d7ab1b',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
