<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>贝叶斯优化原理及应用</title>
    <url>/2020/12/29/BayesianOptimization/</url>
    <content><![CDATA[<p><img src="https://images.bumpchicken.cn/img/20220509122455.png"></p>
<p>贝叶斯优化(Bayesian Optimization, BO)是一种黑盒优化算法，用于求解表达式未知的函数极值问题。算法使用高斯过程回归对一组采样点的函数值进行概率建模，预测出任意点处函数值的概率分布，然后构造采集函数(Acquistion Function)，用于衡量每一个点值得探索(explore)的程度，求解采集函数的极值从而确定下一个采样点，最后返回这组采样点的极值作为函数的极值。</p>
<span id="more"></span>
<h2 id="黑盒优化问题"><a href="#黑盒优化问题" class="headerlink" title="黑盒优化问题"></a>黑盒优化问题</h2><p>训练机器学习模型过程中，会有很多模型参数之外的参数，如学习率，卷积核大小等，再比如训练xgboost时，树的最大深度、采样率等参数都会影响训练结果，这些参数我们将其称为超参数。假设一组超参数组合$X = x<em>{1},x</em>{2},…,x_{n}$，存在一个未知函数 $f:x \rightarrow \mathbb{R}$，我们需要在$x \subseteq X $找到一组最佳参数组合$x^{*}$使得:</p>
<script type="math/tex; mode=display">x^{*} = \underset{x\in X}{argmin} f(x)</script><p>当$f$是凸函数并且定义域也是凸的时候，可以通过凸优化手段(梯度下降、L_BFGS等)来求解。但是超参数优化属于黑盒优化问题，$f$不一定是凸函数，并且是未知的，在优化过程中只能得到函数的输入和输出，不能获取目标函数的表达式和梯度信息，这里的$f$通常还是计算代价非常昂贵的函数，因此优化过程会比较困难，尤其是当超参数数量大的情况。常用的超参数优化方法有网格搜索(Grid Search)，随机搜索(Random Search)，遗传算法（粒子群优化、模拟退火等）以及本文要介绍的贝叶斯优化方法。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AutoML</tag>
      </tags>
  </entry>
  <entry>
    <title>动态归整距离（DTW）详解</title>
    <url>/2019/04/08/DTW/</url>
    <content><![CDATA[<p>DTW(Dynamic Time Warping)动态归整距离是一种衡量两个时间序列相似度的方法</p>
<span id="more"></span>
<h2 id="DTW原理"><a href="#DTW原理" class="headerlink" title="DTW原理"></a>DTW原理</h2><p>在时间序列分析中，需要比较相似性的两段时间序列的长度可能并不相等，在语音识别领域表现为不同人的语速不同。而且同一个单词内的不同音素的发音速度也不同，比如有的人会把‘A’这个音拖得很长，或者把‘i’发的很短。另外，不同时间序列可能仅仅存在时间轴上的位移，亦即在还原位移的情况下，两个时间序列是一致的。在这些复杂情况下，使用传统的欧几里得距离无法有效地求的两个时间序列之间的距离(或者相似性)。</p>
<p><strong>DTW通过把时间序列进行延伸和缩短，来计算两个时间序列性之间的相似性</strong>：</p>
<p><img src="https://images.bumpchicken.cn/img/20220509012950.png" width="50%" height="50%"></p>
<p>如上图所示，上下两条实线代表两个时间序列，它们之间的虚线代表两个时间序列之间相似的点。DTW使用所有这些相似点之间距离的和，称之为归整路径距离（Warp Path Distance)</p>
<h2 id="DTW计算方法"><a href="#DTW计算方法" class="headerlink" title="DTW计算方法"></a>DTW计算方法</h2><p>令要计算相似度的两个时间序列分别为$X$和$Y$，长度分别为$|X|$和$|Y|$</p>
<h3 id="归整路径-Warp-Path"><a href="#归整路径-Warp-Path" class="headerlink" title="归整路径(Warp Path)"></a>归整路径(Warp Path)</h3><p>归整路径的形式为$W=w<em>{1},w</em>{2},…,w<em>{k}$,其中$Max(|x|,|Y|)&lt;= K &lt;= |X| + |Y|$<br>$w</em>{k}$的形式为$(i,j)$，其中$i$表示的是$X$中的$i$坐标，$j$表示的是$Y$中的$j$坐标。<br>归整路径$W$必须从$w<em>{1}=(1,1)$ 开始，到$w</em>{k}=(|X|,|Y|)$结尾，以保证$X$和$Y$中的每个坐标都在$W$中出现。<br>另外，$w_{k}$中$(i,j)$必须是单调增加的，以保证上图中的虚线不会相交，所谓单调增加是指：</p>
<script type="math/tex; mode=display">w_{k}=(i,j), w_{k+1}=(i^{'},j^{'}) \qquad i<=i^{'}<=i+1, j<=j^{'}<=j+1</script><p>最后得到的归整路径是距离最短的一个路径:</p>
<script type="math/tex; mode=display">Dist(W)=\sum^{k=K}_{k=1}Dist(w_{ki}, w_{kj})</script><p>其中$Dist(w<em>{ki}, w</em>{kj}$为任意经典的距离计算方法，比如欧氏距离。$w<em>{ki}$是指$X$的第i个数据点，$w</em>{kj}$是指$Y$的第$j$个数据点</p>
<h3 id="DTW实现"><a href="#DTW实现" class="headerlink" title="DTW实现"></a>DTW实现</h3><p>在实现DTW时，我们采用动态规划的思想，令$D(i,j)$表示长度为$i$和$j$的两个时间序列之间的归整路径距离:</p>
<script type="math/tex; mode=display">D(i,j)=Dist(i,j)+min[D(i-1,j),D(i,j-1),D(i-1,j-1)]</script><p><img src="https://images.bumpchicken.cn/img/20220509015951.png" width="80%" height="30%"></p>
<p>代码如下:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">distance</span>(<span class="params">x,y</span>):</span><br><span class="line">   <span class="string">&quot;&quot;&quot;定义你的距离函数，欧式距离，街区距离等等&quot;&quot;&quot;</span></span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">abs</span>(x-y)</span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dtw</span>(<span class="params">X,Y</span>):</span><br><span class="line">    M=[[distance(X[i],Y[j]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X))] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(Y))]</span><br><span class="line">    l1=<span class="built_in">len</span>(X)</span><br><span class="line">    l2=<span class="built_in">len</span>(Y) </span><br><span class="line">    D=[[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(l1+<span class="number">1</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(l2+<span class="number">1</span>)]</span><br><span class="line">    D[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,l1+<span class="number">1</span>):</span><br><span class="line">      D[i][<span class="number">0</span>]=sys.maxint</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,l2+<span class="number">1</span>):</span><br><span class="line">      D[<span class="number">0</span>][j]=sys.maxint</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,l1+<span class="number">1</span>):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,l2+<span class="number">1</span>):</span><br><span class="line">        D[i][j]=M[i-<span class="number">1</span>][j-<span class="number">1</span>]+<span class="built_in">min</span>(D[i-<span class="number">1</span>][j],D[i][j-<span class="number">1</span>],D[i-<span class="number">1</span>][j-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> D[l1][l2]</span><br></pre></td></tr></table></figure></p>
<p><strong>DTW采用动态规划实现，时间复杂度为$O(N^{2})$,有一些改进的快速DTW算法，如FastDTW[1], SparseDTW, LB_Keogh, LB_Imporved等</strong></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>FastDTW: Toward Accurate Dynamic Time Warping in Linear Time and Space. Stan Salvador, Philip Chan.</li>
</ol>
]]></content>
      <categories>
        <category>时间序列分析</category>
      </categories>
      <tags>
        <tag>时间序列分析</tag>
      </tags>
  </entry>
  <entry>
    <title>DBSCAN算法原理</title>
    <url>/2018/09/25/Dbscan/</url>
    <content><![CDATA[<p><img src="https://images.bumpchicken.cn/img/20220508173927.png" width="60%" height="20%"></p>
<p>DBSCAN(Density-Based Spatial Clustering of Application with Noise)是一种基于密度的空间聚类算法。该算法将具有足够密度的区域划分为簇，并在具有噪声的空间数据中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。算法无需事先指定聚类中心数目，可以对大规模无规则形状的数据进行有效聚类</p>
<span id="more"></span>
<h2 id="相关定义"><a href="#相关定义" class="headerlink" title="相关定义"></a>相关定义</h2><p>DBSCAN有自己的一套符号体系，定义了许多新概念，数学体系十分严谨</p>
<h3 id="密度定义"><a href="#密度定义" class="headerlink" title="密度定义"></a>密度定义</h3><p>给定数据集$D$</p>
<ul>
<li>$\epsilon$: 邻域半径</li>
<li>$\epsilon$-邻域: 邻域内点的集合</li>
</ul>
<script type="math/tex; mode=display">N_{\varepsilon}(p):=\{\text{q in dataset D} \mid \operatorname{dist}(p,q)<=\varepsilon\}</script><p>【注】<em>距离度量$dist(p,q)$是聚类算法中一个值得探究的问题。此处的距离度量可以为欧氏距离、曼哈顿距离等多种距离度量方式，并且数据点的维度可为任意维度</em></p>
<ul>
<li>MinPts: 核心点邻域内数据点的最小数量</li>
</ul>
<p><img src="https://images.bumpchicken.cn/img/20220508215638.png" width="50%" height="80%"></p>
<p>如上图，当MinPts = 4, p的密度相较于q大，p称为高密度点</p>
<h3 id="核心点、边界点和离群点定义"><a href="#核心点、边界点和离群点定义" class="headerlink" title="核心点、边界点和离群点定义"></a>核心点、边界点和离群点定义</h3><p><img src="https://images.bumpchicken.cn/img/20220508220041.png" width="60%" height="60%"></p>
<ul>
<li>核心点(Core): 高密度点，其 $\epsilon$-邻域数据点数量 &gt;= MinPts</li>
<li>边界点(Border): 低密度点，但在某个核心点的邻域内</li>
<li>离群点(Outlier): 既不是核心点也不是边界点</li>
</ul>
<h3 id="密度可达定义"><a href="#密度可达定义" class="headerlink" title="密度可达定义"></a>密度可达定义</h3><ul>
<li>直接密度可达： 如果p是一个核心点，切q在p的$\epsilon$-邻域内，那么称q直接密度可达p</li>
</ul>
<p>【注】<em>不能说p直接密度可达q，直接密度可达不具有对称性(symmetric)</em></p>
<p><img src="https://images.bumpchicken.cn/img/20220508221408.png" width="40%" height="40%"></p>
<ul>
<li>密度可达:如果存在一串这样的数据点: $p<em>{1},p</em>{2},…,p<em>{n}$，其中$p</em>{1}=q,p<em>{n}=p$,且$p</em>{i+1}$直接密度可达$p_{i}$，那么称p密度可达q</li>
</ul>
<p>【注】<em>不能说q密度可达p，密度可达同样不具有对称性</em></p>
<p><img src="https://images.bumpchicken.cn/img/20220508222514.png" width="50%" height="50%"></p>
<h3 id="密度连通"><a href="#密度连通" class="headerlink" title="密度连通"></a>密度连通</h3><p>如果p和q都密度可达点o，那么称p和q密度连通，如下所示</p>
<p><img src="https://images.bumpchicken.cn/img/20220508224900.png" width="50%" height="50%"></p>
<p>【注】<em>密度连通具有对称性，可以说q和p密度连通</em></p>
<h2 id="聚类准则"><a href="#聚类准则" class="headerlink" title="聚类准则"></a>聚类准则</h2><p>给定一个数据集D，参数$\epsilon$和MinPts，那么聚类产生的子集C必须满足两个准则：</p>
<ol>
<li>Maximality(极大性)：对于任意的p、q，如果$p\in C$，且q密度可达p，那么同样$q\in C$</li>
<li>Connectivity(连通性)：对于任意的p、q，p和q是密度相连的</li>
</ol>
<h2 id="聚类流程"><a href="#聚类流程" class="headerlink" title="聚类流程"></a>聚类流程</h2><p>DBSCAN聚类过程如下图所示</p>
<p><img src="https://images.bumpchicken.cn/img/20220508225615.png" width="80%" height="80%"></p>
<h2 id="参数选择"><a href="#参数选择" class="headerlink" title="参数选择"></a>参数选择</h2><h3 id="邻域大小-epsilon"><a href="#邻域大小-epsilon" class="headerlink" title="邻域大小$\epsilon$"></a>邻域大小$\epsilon$</h3><p>DBSCAN采用全局$\epsilon$和MinPts值，因此每个节点的邻域大小是一致的。当数据密度和聚簇间距离分布不均匀时，若选取较小的$\epsilon$，则较稀疏的聚簇中的数据点密度会小于MintPts，会被认为是边界点而不被用于所在类的进一步扩展，可能导致较稀疏的聚簇被划分为多个性质相似的小聚簇。相反,若选取较大的$\epsilon$，则离得较近而密度较大的那些聚簇可能被合并为同一个聚簇，他们之间的差异将被忽略。因此这种情况下，选取合适的邻域大小是较为困难的，当维度较高时，$\epsilon$的选取更加困难</p>
<h3 id="MinPts"><a href="#MinPts" class="headerlink" title="MinPts"></a>MinPts</h3><p>参数MinPts的选取有一个指导性原则，即 $MinPts &gt;= dim+1$，这里$dim$表示聚类空间的维度大小</p>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol>
<li>可以对任意形状的稠密数据集进行聚类（K-Means一般只适用于凸数据集）</li>
<li>可以在聚类时发现异常点，对数据集的异常点不敏感</li>
</ol>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol>
<li>如果样本集的密度不均匀，聚类间距相距很大时，聚类效果较差</li>
<li>对于参数 $\epsilon$ 和MinPts敏感，不同参数组合对聚类效果影响较大</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title>孤立森林原理详解</title>
    <url>/2020/05/25/IsolationTree/</url>
    <content><![CDATA[<p><img src="https://images.bumpchicken.cn/img/tree.png" alt=""></p>
<p>孤立森林（Isolation Forest）是周志华团队于2008年提出的一种具有线性复杂度的异常检测算法，被工业界广泛应用于诸如异常流量检测,金融欺诈行为检测等场景。</p>
<span id="more"></span>
<h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p>异常检测领域，通常是正常的样本占大多数，离群点占绝少数，因此大多数异常检测算法的基本思想都是对正常点构建模型，然后根据规则识别出不属于正常点模型的离群点，比较典型的算法有One Class SVM(OCSVM), Local Outlier Factor(LOF)。和多数异常检测算法不同，孤立森林采用了一种较为高效的异常发现算法，其思路很朴素，但也足够直观有效。</p>
<p>考虑以下场景，一个二维平面上零零散散分布着一些点，随机使用分割线对其进行分割，直至所有但点都不可再划分（即被孤立了）。直观上来讲，可以发现那些密度很高的簇需要被切割很多次才会停止切割，但是密度很低的点很快就会停止切割到某个子空间了。</p>
<p><img src="https://images.bumpchicken.cn/img/20220424235501.png" width="90%" height="50%"></p>
<p>孤立森林分<b>训练</b>和<b>异常评估</b>两部分:</p>
<ul>
<li><b>训练:</b> 根据样本抽样构建多棵iTree，形成孤立森林</li>
<li><b>异常评估:</b> 根据训练过程构建的孤立森林，计算待评估值的异常得分</li>
</ul>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><ol>
<li>给定训练数据集$X$，确定需要构建的孤立树（iTree）个数t，按$\phi$采样大小随机取样作为子样本集$X^{‘}$</li>
<li>在子样本集$X^{‘}$上构建一棵孤立树(iTree)，过程如下图所示：</li>
</ol>
<p><img src="https://images.bumpchicken.cn/img/20220508001729.png" width="80%" height="20%"></p>
<p>a) 在$X$中随机选择一个属性（维度），在当前样本数据范围内，随机产生一个分割点$p$(介于当前维度最大和最小值之间)</p>
<p>b) 此切割点即是一个超平面，将当前节点数据空间切分成2个子空间：将当前所选维度下小于p点的放在当前节点左分支，把大于p点的放在当前节点的右分支</p>
<p>c) 在节点的左分支和右分支递归执行步骤a，b，不断构造新的叶子节点，直到叶子节点上只有一个数据或者树已经生长到了限制的高度</p>
<p>d) 单棵iTree构建完成</p>
<ol>
<li>按2的过程，依次构建t棵iTree，得到孤立森林</li>
</ol>
<p><img src="https://images.bumpchicken.cn/img/20220508003153.png" width="80%" height="30%"></p>
<h2 id="异常评估"><a href="#异常评估" class="headerlink" title="异常评估"></a>异常评估</h2><p>构建了孤立森林(IForest)后，可以评估某个点$x$的异常得分，用到如下公式:</p>
<script type="math/tex; mode=display">s(x,n)=2^{-\frac{E(h(x))}{c(n}}</script><p>其中，$h(x)$ 表示$x$在某棵孤立树中的路径长度，$E(h(x))$ 表示在所有孤立树中的期望路径长度。$c(n)$ 为样本数为n时的二叉排序树(BST)的平均搜索路径长度，用来对样本$x$的期望路径长度做归一化处理。$c(n)$公式如下:</p>
<script type="math/tex; mode=display">c(n)=2H(n-1)-(2(n-1)/n)</script><p>其中，$H(i)$是一个调和数，约等于 $ln(i) + \gamma$，$\gamma$为欧拉常数，约等于0.5772156649</p>
<p>论文对于异常得分分布有如下结论：</p>
<ol>
<li><p>如果异常得分接近1，那么一定是异常点</p>
</li>
<li><p>如果异常得分远小于0.5, 那么一定不是异常点</p>
</li>
<li><p>如果样本点的异常得分均在0.5左右，那么样本中可能不存在异常点</p>
</li>
</ol>
<p>异常得分$s$和$E(h(x))$关系图如下所示</p>
<p><img src="https://images.bumpchicken.cn/img/20220508010609.png" width="50%" height="30%"></p>
<p>异常得分的等高线图如下所示，通常潜在的异常点$s&gt;=0.6$<br><img src="http://images.bumpchicken.cn/img/20220508010909.png" width="50%" height="30%"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.Liu F T, Ting K M, Zhou Z H. Isolation forest[C]//2008 Eighth IEEE International Conference on Data Mining. IEEE, 2008: 413-422.</p>
]]></content>
      <categories>
        <category>异常检测</category>
      </categories>
      <tags>
        <tag>异常检测</tag>
      </tags>
  </entry>
  <entry>
    <title>PageRank算法原理</title>
    <url>/2020/12/08/PageRank/</url>
    <content><![CDATA[<p>PageRank, 网页排名，又称网页级别，佩奇排名等，是一种由搜索引擎根据网页之间相互的超链接计算的技术，作为网页排名的要素之一，以Google创办人拉里佩奇(Larry Page)命名。Google用它来体现网页的相关性和重要性，在搜索引擎优化操作中是经常被用来评估网页优化的成效因素之一。</p>
<span id="more"></span>
<h2 id="PageRank简单形式"><a href="#PageRank简单形式" class="headerlink" title="PageRank简单形式"></a>PageRank简单形式</h2><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><ul>
<li>如果一个网页被其他很多网页链接，则重要，权值高</li>
<li>如果PageRank值高的网页链接某个网页，则该网页权值也会相应提高</li>
</ul>
<h3 id="计算方式"><a href="#计算方式" class="headerlink" title="计算方式"></a>计算方式</h3><p>假设有如下四个网页A、B、C、D，链接信息如下图所示:</p>
<p><img src="https://images.bumpchicken.cn/img/20220508234438.png" width="50%" height="50%"></p>
<p>上图是一个有向图，将网页看成节点，网页之间的链接关系用边表示，出链指的是链接出去的链接，入链指的是进来的链接，比如上图A有2个入链，3个出链</p>
<p><strong>PageRank定义</strong>: 一个网页的影响力 = 所有入链集合的页面加权影响力之和</p>
<p>上图A节点的影响力可用如下公式计算:</p>
<script type="math/tex; mode=display">PR(A) = \frac{PR(B)}{L(B)} + \frac{PR(C)}{L(C)} + \frac{PR(D)}{L(D)}</script><p>其中，$PR(A)$表示网页A的影响力，$L(B)$表示B的出链数量，用通用的公式表示为：</p>
<script type="math/tex; mode=display">PR(u)=\sum_{\nu \in B_{u}} \frac{P R(v)}{L(v)}</script><p>u为待评估的页面，$B_{u}$为页面u的入链集合。针对入链集合中的任意页面v，它能给u带来的影响力是其自身的影响力$PR(v)$除以v页面的出链数量，即页面v把影响力$PR(v$平均分配给了它的出链，这样统计所有能给u带来链接的页面v，得到的总和就是网页u的影响力，即为$PR(u)$</p>
<p>因此，PageRank的简单形式定义如下：</p>
<blockquote>
<p>当含有若干个节点的有向图是强连通且非周期性的有向图时，在其基础上定义的随机游走模型，即一阶马尔科夫链具有平稳分布，平稳分布向量称为这个有向图的PageRank。若矩阵M是马尔科夫链的转移矩阵，则向量R满足:</p>
<script type="math/tex; mode=display">MR = R</script></blockquote>
<p>上图A、B、C、D四个网页的转移矩阵M如下:</p>
<script type="math/tex; mode=display">M=\left[\begin{array}{cccc}
0 & 1 / 2 & 1 & 0 \\
1 / 3 & 0 & 0 & 1 / 2 \\
1 / 3 & 0 & 0 & 1 / 2 \\
1 / 3 & 1 / 2 & 0 & 0
\end{array}\right]</script><p>假设A、B、C、D四个页面的初始影响力是相同的，即$w_{0}^{T} = [1/4\space1/4\space1/4\space1/4]$</p>
<p>第一次转移后，各页面影响力$w_{1}$变为:</p>
<script type="math/tex; mode=display">w_{1}=M w_{0}=\left[\begin{array}{cccc}
0 & 1 / 2 & 1 & 0 \\
1 / 3 & 0 & 0 & 1 / 2 \\
1 / 3 & 0 & 0 & 1 / 2 \\
1 / 3 & 1 / 2 & 0 & 0
\end{array}\right]\left[\begin{array}{c}
1 / 4 \\
1 / 4 \\
1 / 4 \\
1 / 4
\end{array}\right]=\left[\begin{array}{l}
9 / 24 \\
5 / 24 \\
5 / 24 \\
5 / 24
\end{array}\right]</script><p>之后再用转移矩阵乘以$w<em>{1}$得到$w</em>{2}$，直到第n次迭代后$w_{n}$收敛不再变化，上述例子，$w$会收敛至[0.3333 0.2222 0.2222 0.2222]，对应A、B、C、D的影响力</p>
<h3 id="等级泄露和等级沉没"><a href="#等级泄露和等级沉没" class="headerlink" title="等级泄露和等级沉没"></a>等级泄露和等级沉没</h3><ol>
<li>等级泄露（Rank Leak): 如果一个网页没有出链，就像是一个黑洞一样，吸收了其他网页的影响力而不释放，最终会导致其他网页的PR值为0，如下图所示:</li>
</ol>
<p><img src="https://images.bumpchicken.cn/img/20220509000856.png" width="50%" height="50%"></p>
<ol>
<li>等级沉没（Rank Sink): 如果一个网页只有出链没有入链，计算过程迭代下来，会导致这个网页的PR值为0，入下图所示:</li>
</ol>
<p><img src="https://images.bumpchicken.cn/img/20220509001111.png" width="50%" height="50%"></p>
<h2 id="PageRank改进版"><a href="#PageRank改进版" class="headerlink" title="PageRank改进版"></a>PageRank改进版</h2><p>为了解决简化模型中存在的等级泄露和等级沉没问题，拉里佩奇提出了PageRank的随机浏览模型。他假设了这样一个场景：</p>
<blockquote>
<p>用户并不都是按照跳转链接的方式来上网，还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意页面，比如用户就是要直接输入网址访问其他页面，虽然这个概率比较小</p>
</blockquote>
<p>所以他定义了阻尼因子d，这个因子代表了用户按照跳转链接来上网的概率，通常可以取一个固定值0.85，而$1-d=0.15$则代表了用户不是通过跳转链接的方式来访问网页的概率</p>
<p>下式是PageRank计算影响力的改进公式:</p>
<script type="math/tex; mode=display">PR(u)=\frac{1-d}{N}+d \sum_{\nu=B_{u}} \frac{P R(v)}{L(v)}</script><p>其中，N为网页总数，这样我们有可以重新迭代网页的权重计算了，因为加入了阻尼因子d，一定程度上解决了等级泄露和等级沉没的问题</p>
<p>同样地，定义概率转移矩阵M，则其一般公式如下:</p>
<script type="math/tex; mode=display">R=d M R+\frac{1-d}{n}1</script><p>其中，$d(0&lt;=d&lt;=1)$为阻尼因子，$1$是所有分量为1的n维向量</p>
<h2 id="PageRank-代码实现"><a href="#PageRank-代码实现" class="headerlink" title="PageRank 代码实现"></a>PageRank 代码实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csc_matrix</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pageRank</span>(<span class="params">G, s=<span class="number">.85</span>, maxerr=<span class="number">.0001</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes the pagerank for each of the n states</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    G: matrix representing state transitions</span></span><br><span class="line"><span class="string">       Gij is a binary value representing a transition from state i to j.</span></span><br><span class="line"><span class="string">    s: probability of following a transition. 1-s probability of teleporting</span></span><br><span class="line"><span class="string">       to another state.</span></span><br><span class="line"><span class="string">    maxerr: if the sum of pageranks between iterations is bellow this we will</span></span><br><span class="line"><span class="string">            have converged.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n = G.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># transform G into markov matrix A</span></span><br><span class="line">    A = csc_matrix(G, dtype=np.<span class="built_in">float</span>)</span><br><span class="line">    rsums = np.array(A.<span class="built_in">sum</span>(<span class="number">1</span>))[:, <span class="number">0</span>]</span><br><span class="line">    ri, ci = A.nonzero()</span><br><span class="line">    A.data /= rsums[ri]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># bool array of sink states</span></span><br><span class="line">    sink = rsums == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute pagerank r until we converge</span></span><br><span class="line">    ro, r = np.zeros(n), np.ones(n)</span><br><span class="line">    <span class="keyword">while</span> np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(r - ro)) &gt; maxerr:       <span class="comment"># 迭代直至收敛</span></span><br><span class="line">        ro = r.copy()</span><br><span class="line">        <span class="comment"># calculate each pagerank at a time</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n):</span><br><span class="line">            <span class="comment"># inlinks of state i</span></span><br><span class="line">            Ai = np.array(A[:, i].todense())[:, <span class="number">0</span>]</span><br><span class="line">            <span class="comment"># account for sink states</span></span><br><span class="line">            Di = sink / <span class="built_in">float</span>(n)</span><br><span class="line">            <span class="comment"># account for teleportation to state i</span></span><br><span class="line">            Ei = np.ones(n) / <span class="built_in">float</span>(n)</span><br><span class="line">            r[i] = ro.dot(Ai * s + Di * s + Ei * (<span class="number">1</span> - s))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># return normalized pagerank</span></span><br><span class="line">    <span class="keyword">return</span> r / <span class="built_in">float</span>(<span class="built_in">sum</span>(r))</span><br></pre></td></tr></table></figure>
<p>使用示例：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">G = np.array([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(pageRank(G,s=<span class="number">.86</span>))</span><br><span class="line">--------------------</span><br><span class="line">[<span class="number">0.12727557</span> <span class="number">0.03616954</span> <span class="number">0.12221594</span> <span class="number">0.22608452</span> <span class="number">0.28934412</span> <span class="number">0.03616954</span> <span class="number">0.16274076</span>]</span><br></pre></td></tr></table></figure></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://www.cnblogs.com/jpcflyer/p/11180263.html">https://www.cnblogs.com/jpcflyer/p/11180263.html</a></p>
<p>2.<a href="https://github.com/fengdu78/lihang-code/blob/master/%E7%AC%AC21%E7%AB%A0%20PageRank%E7%AE%97%E6%B3%95/21.PageRank.ipynb">PageRank notebook</a></p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>一些相似算法或改进的算法</p>
<p>1.LeaderRank</p>
<p>2.Hilltop算法</p>
<p>3.ExpertRank</p>
<p>4.HITS</p>
<p>5.TrustRank</p>
]]></content>
      <categories>
        <category>根因定位</category>
      </categories>
      <tags>
        <tag>排序</tag>
      </tags>
  </entry>
</search>
