<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>孤立森林原理详解</title>
    <url>/2020/05/25/IsolationTree/</url>
    <content><![CDATA[<p><img src="https://images.bumpchicken.cn/img/tree.png" /></p>
<p>孤立森林（Isolation
Forest）是周志华团队于2008年提出的一种具有线性复杂度的异常检测算法，被工业界广泛应用于诸如异常流量检测,金融欺诈行为检测等场景。</p>
<span id="more"></span>
<h2 id="算法原理">算法原理</h2>
<p>异常检测领域，通常是正常的样本占大多数，离群点占绝少数，因此大多数异常检测算法的基本思想都是对正常点构建模型，然后根据规则识别出不属于正常点模型的离群点，比较典型的算法有One
Class SVM(OCSVM), Local Outlier
Factor(LOF)。和多数异常检测算法不同，孤立森林采用了一种较为高效的异常发现算法，其思路很朴素，但也足够直观有效。</p>
<p>考虑以下场景，一个二维平面上零零散散分布着一些点，随机使用分割线对其进行分割，直至所有但点都不可再划分（即被孤立了）。直观上来讲，可以发现那些密度很高的簇需要被切割很多次才会停止切割，但是密度很低的点很快就会停止切割到某个子空间了。</p>
<p><img src="https://images.bumpchicken.cn/img/20220424235501.png" width="90%" height="50%"></p>
<p>孤立森林分<b>训练</b>和<b>异常评估</b>两部分:</p>
<ul>
<li><b>训练:</b> 根据样本抽样构建多棵iTree，形成孤立森林</li>
<li><b>异常评估:</b>
根据训练过程构建的孤立森林，计算待评估值的异常得分</li>
</ul>
<h2 id="训练">训练</h2>
<ol type="1">
<li>给定训练数据集<span
class="math inline">\(X\)</span>，确定需要构建的孤立树（iTree）个数t，按<span
class="math inline">\(\phi\)</span>采样大小随机取样作为子样本集<span
class="math inline">\(X^{&#39;}\)</span></li>
<li>在子样本集<span
class="math inline">\(X^{&#39;}\)</span>上构建一棵孤立树(iTree)，过程如下图所示：</li>
</ol>
<p><img src="https://images.bumpchicken.cn/img/20220508001729.png" width="80%" height="20%"></p>
<ol type="a">
<li><p>在<span
class="math inline">\(X\)</span>中随机选择一个属性（维度），在当前样本数据范围内，随机产生一个分割点<span
class="math inline">\(p\)</span>(介于当前维度最大和最小值之间)</p></li>
<li><p>此切割点即是一个超平面，将当前节点数据空间切分成2个子空间：将当前所选维度下小于p点的放在当前节点左分支，把大于p点的放在当前节点的右分支</p></li>
<li><p>在节点的左分支和右分支递归执行步骤a，b，不断构造新的叶子节点，直到叶子节点上只有一个数据或者树已经生长到了限制的高度</p></li>
<li><p>单棵iTree构建完成</p></li>
</ol>
<ol start="3" type="1">
<li>按2的过程，依次构建t棵iTree，得到孤立森林</li>
</ol>
<p><img src="https://images.bumpchicken.cn/img/20220508003153.png" width="80%" height="30%"></p>
<h2 id="异常评估">异常评估</h2>
<p>构建了孤立森林(IForest)后，可以评估某个点<span
class="math inline">\(x\)</span>的异常得分，用到如下公式:</p>
<p><span class="math display">\[s(x,n)=2^{-\frac{E(h(x))}{c(n}}
\]</span></p>
<p>其中，<span class="math inline">\(h(x)\)</span> 表示<span
class="math inline">\(x\)</span>在某棵孤立树中的路径长度，<span
class="math inline">\(E(h(x))\)</span>
表示在所有孤立树中的期望路径长度。<span
class="math inline">\(c(n)\)</span>
为样本数为n时的二叉排序树(BST)的平均搜索路径长度，用来对样本<span
class="math inline">\(x\)</span>的期望路径长度做归一化处理。<span
class="math inline">\(c(n)\)</span>公式如下:</p>
<p><span class="math display">\[c(n)=2H(n-1)-(2(n-1)/n)\]</span></p>
<p>其中，<span class="math inline">\(H(i)\)</span>是一个调和数，约等于
<span class="math inline">\(ln(i) + \gamma\)</span>，<span
class="math inline">\(\gamma\)</span>为欧拉常数，约等于0.5772156649</p>
<p>论文对于异常得分分布有如下结论：</p>
<ol type="1">
<li><p>如果异常得分接近1，那么一定是异常点</p></li>
<li><p>如果异常得分远小于0.5, 那么一定不是异常点</p></li>
<li><p>如果样本点的异常得分均在0.5左右，那么样本中可能不存在异常点</p></li>
</ol>
<p>异常得分<span class="math inline">\(s\)</span>和<span
class="math inline">\(E(h(x))\)</span>关系图如下所示</p>
<p><img src="https://images.bumpchicken.cn/img/20220508010609.png" width="50%" height="30%"></p>
<p>异常得分的等高线图如下所示，通常潜在的异常点<span
class="math inline">\(s&gt;=0.6\)</span>
<img src="http://images.bumpchicken.cn/img/20220508010909.png" width="50%" height="30%"></p>
<h2 id="参考资料">参考资料</h2>
<p>1.Liu F T, Ting K M, Zhou Z H. Isolation forest[C]//2008 Eighth IEEE
International Conference on Data Mining. IEEE, 2008: 413-422.</p>
]]></content>
      <categories>
        <category>异常检测</category>
      </categories>
      <tags>
        <tag>异常检测</tag>
      </tags>
  </entry>
  <entry>
    <title>DBSCAN算法原理</title>
    <url>/2018/09/25/Dbscan/</url>
    <content><![CDATA[<p><img src="https://images.bumpchicken.cn/img/20220508173927.png" width="60%" height="20%"></p>
<p>DBSCAN(Density-Based Spatial Clustering of Application with
Noise)是一种基于密度的空间聚类算法。该算法将具有足够密度的区域划分为簇，并在具有噪声的空间数据中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。算法无需事先指定聚类中心数目，可以对大规模无规则形状的数据进行有效聚类</p>
<span id="more"></span>
<h2 id="相关定义">相关定义</h2>
<p>DBSCAN有自己的一套符号体系，定义了许多新概念，数学体系十分严谨</p>
<h3 id="密度定义">密度定义</h3>
<p>给定数据集<span class="math inline">\(D\)</span></p>
<ul>
<li><span class="math inline">\(\epsilon\)</span>: 邻域半径</li>
<li><span class="math inline">\(\epsilon\)</span>-邻域:
邻域内点的集合</li>
</ul>
<p><span class="math display">\[N_{\varepsilon}(p):=\{\text{q in dataset
D} \mid \operatorname{dist}(p,q)&lt;=\varepsilon\}\]</span></p>
<p>【注】<em>距离度量<span
class="math inline">\(dist(p,q)\)</span>是聚类算法中一个值得探究的问题。此处的距离度量可以为欧氏距离、曼哈顿距离等多种距离度量方式，并且数据点的维度可为任意维度</em></p>
<ul>
<li>MinPts: 核心点邻域内数据点的最小数量</li>
</ul>
<p><img src="https://images.bumpchicken.cn/img/20220508215638.png" width="50%" height="80%"></p>
<p>如上图，当MinPts = 4, p的密度相较于q大，p称为高密度点</p>
<h3 id="核心点边界点和离群点定义">核心点、边界点和离群点定义</h3>
<p><img src="https://images.bumpchicken.cn/img/20220508220041.png" width="60%" height="60%"></p>
<ul>
<li>核心点(Core): 高密度点，其 <span
class="math inline">\(\epsilon\)</span>-邻域数据点数量 &gt;= MinPts</li>
<li>边界点(Border): 低密度点，但在某个核心点的邻域内</li>
<li>离群点(Outlier): 既不是核心点也不是边界点</li>
</ul>
<h3 id="密度可达定义">密度可达定义</h3>
<ul>
<li>直接密度可达： 如果p是一个核心点，切q在p的<span
class="math inline">\(\epsilon\)</span>-邻域内，那么称q直接密度可达p</li>
</ul>
<p>【注】<em>不能说p直接密度可达q，直接密度可达不具有对称性(symmetric)</em></p>
<p><img src="https://images.bumpchicken.cn/img/20220508221408.png" width="40%" height="40%"></p>
<ul>
<li>密度可达:如果存在一串这样的数据点: <span
class="math inline">\(p_{1},p_{2},...,p_{n}\)</span>，其中<span
class="math inline">\(p_{1}=q,p_{n}=p\)</span>,且<span
class="math inline">\(p_{i+1}\)</span>直接密度可达<span
class="math inline">\(p_{i}\)</span>，那么称p密度可达q</li>
</ul>
<p>【注】<em>不能说q密度可达p，密度可达同样不具有对称性</em></p>
<p><img src="https://images.bumpchicken.cn/img/20220508222514.png" width="50%" height="50%"></p>
<h3 id="密度连通">密度连通</h3>
<p>如果p和q都密度可达点o，那么称p和q密度连通，如下所示</p>
<p><img src="https://images.bumpchicken.cn/img/20220508224900.png" width="50%" height="50%"></p>
<p>【注】<em>密度连通具有对称性，可以说q和p密度连通</em></p>
<h2 id="聚类准则">聚类准则</h2>
<p>给定一个数据集D，参数<span
class="math inline">\(\epsilon\)</span>和MinPts，那么聚类产生的子集C必须满足两个准则：</p>
<ol type="1">
<li>Maximality(极大性)：对于任意的p、q，如果<span
class="math inline">\(p\in C\)</span>，且q密度可达p，那么同样<span
class="math inline">\(q\in C\)</span></li>
<li>Connectivity(连通性)：对于任意的p、q，p和q是密度相连的</li>
</ol>
<h2 id="聚类流程">聚类流程</h2>
<p>DBSCAN聚类过程如下图所示</p>
<p><img src="https://images.bumpchicken.cn/img/20220508225615.png" width="80%" height="80%"></p>
<h2 id="参数选择">参数选择</h2>
<h3 id="邻域大小epsilon">邻域大小<span
class="math inline">\(\epsilon\)</span></h3>
<p>DBSCAN采用全局<span
class="math inline">\(\epsilon\)</span>和MinPts值，因此每个节点的邻域大小是一致的。当数据密度和聚簇间距离分布不均匀时，若选取较小的<span
class="math inline">\(\epsilon\)</span>，则较稀疏的聚簇中的数据点密度会小于MintPts，会被认为是边界点而不被用于所在类的进一步扩展，可能导致较稀疏的聚簇被划分为多个性质相似的小聚簇。相反,若选取较大的<span
class="math inline">\(\epsilon\)</span>，则离得较近而密度较大的那些聚簇可能被合并为同一个聚簇，他们之间的差异将被忽略。因此这种情况下，选取合适的邻域大小是较为困难的，当维度较高时，<span
class="math inline">\(\epsilon\)</span>的选取更加困难</p>
<h3 id="minpts">MinPts</h3>
<p>参数MinPts的选取有一个指导性原则，即 <span
class="math inline">\(MinPts &gt;= dim+1\)</span>，这里<span
class="math inline">\(dim\)</span>表示聚类空间的维度大小</p>
<h2 id="优缺点">优缺点</h2>
<h3 id="优点">优点</h3>
<ol type="1">
<li>可以对任意形状的稠密数据集进行聚类（K-Means一般只适用于凸数据集）</li>
<li>可以在聚类时发现异常点，对数据集的异常点不敏感</li>
</ol>
<h3 id="缺点">缺点</h3>
<ol type="1">
<li>如果样本集的密度不均匀，聚类间距相距很大时，聚类效果较差</li>
<li>对于参数 <span class="math inline">\(\epsilon\)</span>
和MinPts敏感，不同参数组合对聚类效果影响较大</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title>PageRank算法原理</title>
    <url>/2020/12/08/PageRank/</url>
    <content><![CDATA[<p>PageRank,
网页排名，又称网页级别，佩奇排名等，是一种由搜索引擎根据网页之间相互的超链接计算的技术，作为网页排名的要素之一，以Google创办人拉里佩奇(Larry
Page)命名。Google用它来体现网页的相关性和重要性，在搜索引擎优化操作中是经常被用来评估网页优化的成效因素之一。</p>
<span id="more"></span>
<h2 id="pagerank简单形式">PageRank简单形式</h2>
<h3 id="基本思想">基本思想</h3>
<ul>
<li>如果一个网页被其他很多网页链接，则重要，权值高</li>
<li>如果PageRank值高的网页链接某个网页，则该网页权值也会相应提高</li>
</ul>
<h3 id="计算方式">计算方式</h3>
<p>假设有如下四个网页A、B、C、D，链接信息如下图所示:</p>
<p><img src="https://images.bumpchicken.cn/img/20220508234438.png" width="50%" height="50%"></p>
<p>上图是一个有向图，将网页看成节点，网页之间的链接关系用边表示，出链指的是链接出去的链接，入链指的是进来的链接，比如上图A有2个入链，3个出链</p>
<p><strong>PageRank定义</strong>: 一个网页的影响力 =
所有入链集合的页面加权影响力之和</p>
<p>上图A节点的影响力可用如下公式计算:</p>
<p><span class="math display">\[PR(A) = \frac{PR(B)}{L(B)} +
\frac{PR(C)}{L(C)} + \frac{PR(D)}{L(D)}\]</span></p>
<p>其中，<span
class="math inline">\(PR(A)\)</span>表示网页A的影响力，<span
class="math inline">\(L(B)\)</span>表示B的出链数量，用通用的公式表示为：</p>
<p><span class="math display">\[PR(u)=\sum_{\nu \in B_{u}} \frac{P
R(v)}{L(v)}\]</span></p>
<p>u为待评估的页面，<span
class="math inline">\(B_{u}\)</span>为页面u的入链集合。针对入链集合中的任意页面v，它能给u带来的影响力是其自身的影响力<span
class="math inline">\(PR(v)\)</span>除以v页面的出链数量，即页面v把影响力<span
class="math inline">\(PR(v\)</span>平均分配给了它的出链，这样统计所有能给u带来链接的页面v，得到的总和就是网页u的影响力，即为<span
class="math inline">\(PR(u)\)</span></p>
<p>因此，PageRank的简单形式定义如下：</p>
<blockquote>
<p>当含有若干个节点的有向图是强连通且非周期性的有向图时，在其基础上定义的随机游走模型，即一阶马尔科夫链具有平稳分布，平稳分布向量称为这个有向图的PageRank。若矩阵M是马尔科夫链的转移矩阵，则向量R满足:
<span class="math display">\[ MR = R \]</span></p>
</blockquote>
<p>上图A、B、C、D四个网页的转移矩阵M如下:</p>
<p><span class="math display">\[M=\left[\begin{array}{cccc}
0 &amp; 1 / 2 &amp; 1 &amp; 0 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 1 / 2 &amp; 0 &amp; 0
\end{array}\right]\]</span></p>
<p>假设A、B、C、D四个页面的初始影响力是相同的，即<span
class="math inline">\(w_{0}^{T} =
[1/4\space1/4\space1/4\space1/4]\)</span></p>
<p>第一次转移后，各页面影响力<span
class="math inline">\(w_{1}\)</span>变为:</p>
<p><span class="math display">\[w_{1}=M w_{0}=\left[\begin{array}{cccc}
0 &amp; 1 / 2 &amp; 1 &amp; 0 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 0 &amp; 0 &amp; 1 / 2 \\
1 / 3 &amp; 1 / 2 &amp; 0 &amp; 0
\end{array}\right]\left[\begin{array}{c}
1 / 4 \\
1 / 4 \\
1 / 4 \\
1 / 4
\end{array}\right]=\left[\begin{array}{l}
9 / 24 \\
5 / 24 \\
5 / 24 \\
5 / 24
\end{array}\right]\]</span></p>
<p>之后再用转移矩阵乘以<span
class="math inline">\(w_{1}\)</span>得到<span
class="math inline">\(w_{2}\)</span>，直到第n次迭代后<span
class="math inline">\(w_{n}\)</span>收敛不再变化，上述例子，<span
class="math inline">\(w\)</span>会收敛至[0.3333 0.2222 0.2222
0.2222]，对应A、B、C、D的影响力</p>
<h3 id="等级泄露和等级沉没">等级泄露和等级沉没</h3>
<ol type="1">
<li>等级泄露（Rank Leak):
如果一个网页没有出链，就像是一个黑洞一样，吸收了其他网页的影响力而不释放，最终会导致其他网页的PR值为0，如下图所示:</li>
</ol>
<p><img src="https://images.bumpchicken.cn/img/20220509000856.png" width="50%" height="50%"></p>
<ol start="2" type="1">
<li>等级沉没（Rank Sink):
如果一个网页只有出链没有入链，计算过程迭代下来，会导致这个网页的PR值为0，入下图所示:</li>
</ol>
<p><img src="https://images.bumpchicken.cn/img/20220509001111.png" width="50%" height="50%"></p>
<h2 id="pagerank改进版">PageRank改进版</h2>
<p>为了解决简化模型中存在的等级泄露和等级沉没问题，拉里佩奇提出了PageRank的随机浏览模型。他假设了这样一个场景：</p>
<blockquote>
<p>用户并不都是按照跳转链接的方式来上网，还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意页面，比如用户就是要直接输入网址访问其他页面，虽然这个概率比较小</p>
</blockquote>
<p>所以他定义了阻尼因子d，这个因子代表了用户按照跳转链接来上网的概率，通常可以取一个固定值0.85，而<span
class="math inline">\(1-d=0.15\)</span>则代表了用户不是通过跳转链接的方式来访问网页的概率</p>
<p>下式是PageRank计算影响力的改进公式:</p>
<p><span class="math display">\[PR(u)=\frac{1-d}{N}+d \sum_{\nu=B_{u}}
\frac{P R(v)}{L(v)}\]</span></p>
<p>其中，N为网页总数，这样我们有可以重新迭代网页的权重计算了，因为加入了阻尼因子d，一定程度上解决了等级泄露和等级沉没的问题</p>
<p>同样地，定义概率转移矩阵M，则其一般公式如下:</p>
<p><span class="math display">\[R=d M R+\frac{1-d}{n}1\]</span></p>
<p>其中，<span
class="math inline">\(d(0&lt;=d&lt;=1)\)</span>为阻尼因子，<span
class="math inline">\(1\)</span>是所有分量为1的n维向量</p>
<h2 id="pagerank-代码实现">PageRank 代码实现</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csc_matrix</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pageRank</span>(<span class="params">G, s=<span class="number">.85</span>, maxerr=<span class="number">.0001</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes the pagerank for each of the n states</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    G: matrix representing state transitions</span></span><br><span class="line"><span class="string">       Gij is a binary value representing a transition from state i to j.</span></span><br><span class="line"><span class="string">    s: probability of following a transition. 1-s probability of teleporting</span></span><br><span class="line"><span class="string">       to another state.</span></span><br><span class="line"><span class="string">    maxerr: if the sum of pageranks between iterations is bellow this we will</span></span><br><span class="line"><span class="string">            have converged.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n = G.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># transform G into markov matrix A</span></span><br><span class="line">    A = csc_matrix(G, dtype=np.<span class="built_in">float</span>)</span><br><span class="line">    rsums = np.array(A.<span class="built_in">sum</span>(<span class="number">1</span>))[:, <span class="number">0</span>]</span><br><span class="line">    ri, ci = A.nonzero()</span><br><span class="line">    A.data /= rsums[ri]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># bool array of sink states</span></span><br><span class="line">    sink = rsums == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute pagerank r until we converge</span></span><br><span class="line">    ro, r = np.zeros(n), np.ones(n)</span><br><span class="line">    <span class="keyword">while</span> np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(r - ro)) &gt; maxerr:       <span class="comment"># 迭代直至收敛</span></span><br><span class="line">        ro = r.copy()</span><br><span class="line">        <span class="comment"># calculate each pagerank at a time</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n):</span><br><span class="line">            <span class="comment"># inlinks of state i</span></span><br><span class="line">            Ai = np.array(A[:, i].todense())[:, <span class="number">0</span>]</span><br><span class="line">            <span class="comment"># account for sink states</span></span><br><span class="line">            Di = sink / <span class="built_in">float</span>(n)</span><br><span class="line">            <span class="comment"># account for teleportation to state i</span></span><br><span class="line">            Ei = np.ones(n) / <span class="built_in">float</span>(n)</span><br><span class="line">            r[i] = ro.dot(Ai * s + Di * s + Ei * (<span class="number">1</span> - s))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># return normalized pagerank</span></span><br><span class="line">    <span class="keyword">return</span> r / <span class="built_in">float</span>(<span class="built_in">sum</span>(r))</span><br></pre></td></tr></table></figure>
<p>使用示例： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">G = np.array([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(pageRank(G,s=<span class="number">.86</span>))</span><br><span class="line">--------------------</span><br><span class="line">[<span class="number">0.12727557</span> <span class="number">0.03616954</span> <span class="number">0.12221594</span> <span class="number">0.22608452</span> <span class="number">0.28934412</span> <span class="number">0.03616954</span> <span class="number">0.16274076</span>]</span><br></pre></td></tr></table></figure></p>
<h2 id="参考资料">参考资料</h2>
<p>1.https://www.cnblogs.com/jpcflyer/p/11180263.html</p>
<p>2.<a
href="https://github.com/fengdu78/lihang-code/blob/master/%E7%AC%AC21%E7%AB%A0%20PageRank%E7%AE%97%E6%B3%95/21.PageRank.ipynb">PageRank
notebook</a></p>
<h2 id="其他">其他</h2>
<p>一些相似算法或改进的算法</p>
<p>1.LeaderRank</p>
<p>2.Hilltop算法</p>
<p>3.ExpertRank</p>
<p>4.HITS</p>
<p>5.TrustRank</p>
]]></content>
      <categories>
        <category>根因定位</category>
      </categories>
      <tags>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>动态归整距离（DTW）详解</title>
    <url>/2019/04/08/DTW/</url>
    <content><![CDATA[<p>DTW(Dynamic Time
Warping)动态归整距离是一种衡量两个时间序列相似度的方法</p>
<span id="more"></span>
<h2 id="dtw原理">DTW原理</h2>
<p>在时间序列分析中，需要比较相似性的两段时间序列的长度可能并不相等，在语音识别领域表现为不同人的语速不同。而且同一个单词内的不同音素的发音速度也不同，比如有的人会把‘A’这个音拖得很长，或者把‘i’发的很短。另外，不同时间序列可能仅仅存在时间轴上的位移，亦即在还原位移的情况下，两个时间序列是一致的。在这些复杂情况下，使用传统的欧几里得距离无法有效地求的两个时间序列之间的距离(或者相似性)。</p>
<p><strong>DTW通过把时间序列进行延伸和缩短，来计算两个时间序列性之间的相似性</strong>：</p>
<p><img src="https://images.bumpchicken.cn/img/20220509012950.png" width="50%" height="50%"></p>
<p>如上图所示，上下两条实线代表两个时间序列，它们之间的虚线代表两个时间序列之间相似的点。DTW使用所有这些相似点之间距离的和，称之为归整路径距离（Warp
Path Distance)</p>
<h2 id="dtw计算方法">DTW计算方法</h2>
<p>令要计算相似度的两个时间序列分别为<span
class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>，长度分别为<span
class="math inline">\(|X|\)</span>和<span
class="math inline">\(|Y|\)</span></p>
<h3 id="归整路径warp-path">归整路径(Warp Path)</h3>
<p>归整路径的形式为<span
class="math inline">\(W=w_{1},w_{2},...,w_{k}\)</span>,其中<span
class="math inline">\(Max(|x|,|Y|)&lt;= K &lt;= |X| + |Y|\)</span> <span
class="math inline">\(w_{k}\)</span>的形式为<span
class="math inline">\((i,j)\)</span>，其中<span
class="math inline">\(i\)</span>表示的是<span
class="math inline">\(X\)</span>中的<span
class="math inline">\(i\)</span>坐标，<span
class="math inline">\(j\)</span>表示的是<span
class="math inline">\(Y\)</span>中的<span
class="math inline">\(j\)</span>坐标。 归整路径<span
class="math inline">\(W\)</span>必须从<span
class="math inline">\(w_{1}=(1,1)\)</span> 开始，到<span
class="math inline">\(w_{k}=(|X|,|Y|)\)</span>结尾，以保证<span
class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>中的每个坐标都在<span
class="math inline">\(W\)</span>中出现。 另外，<span
class="math inline">\(w_{k}\)</span>中<span
class="math inline">\((i,j)\)</span>必须是单调增加的，以保证上图中的虚线不会相交，所谓单调增加是指：</p>
<p><span class="math display">\[w_{k}=(i,j),
w_{k+1}=(i^{&#39;},j^{&#39;}) \qquad i&lt;=i^{&#39;}&lt;=i+1,
j&lt;=j^{&#39;}&lt;=j+1\]</span></p>
<p>最后得到的归整路径是距离最短的一个路径:</p>
<p><span class="math display">\[Dist(W)=\sum^{k=K}_{k=1}Dist(w_{ki},
w_{kj})\]</span></p>
<p>其中<span class="math inline">\(Dist(w_{ki},
w_{kj}\)</span>为任意经典的距离计算方法，比如欧氏距离。<span
class="math inline">\(w_{ki}\)</span>是指<span
class="math inline">\(X\)</span>的第i个数据点，<span
class="math inline">\(w_{kj}\)</span>是指<span
class="math inline">\(Y\)</span>的第<span
class="math inline">\(j\)</span>个数据点</p>
<h3 id="dtw实现">DTW实现</h3>
<p>在实现DTW时，我们采用动态规划的思想，令<span
class="math inline">\(D(i,j)\)</span>表示长度为<span
class="math inline">\(i\)</span>和<span
class="math inline">\(j\)</span>的两个时间序列之间的归整路径距离:</p>
<p><span
class="math display">\[D(i,j)=Dist(i,j)+min[D(i-1,j),D(i,j-1),D(i-1,j-1)]\]</span></p>
<p><img src="https://images.bumpchicken.cn/img/20220509015951.png" width="80%" height="30%"></p>
<p>代码如下: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">distance</span>(<span class="params">x,y</span>):</span><br><span class="line">   <span class="string">&quot;&quot;&quot;定义你的距离函数，欧式距离，街区距离等等&quot;&quot;&quot;</span></span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">abs</span>(x-y)</span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dtw</span>(<span class="params">X,Y</span>):</span><br><span class="line">    M=[[distance(X[i],Y[j]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X))] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(Y))]</span><br><span class="line">    l1=<span class="built_in">len</span>(X)</span><br><span class="line">    l2=<span class="built_in">len</span>(Y) </span><br><span class="line">    D=[[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(l1+<span class="number">1</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(l2+<span class="number">1</span>)]</span><br><span class="line">    D[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,l1+<span class="number">1</span>):</span><br><span class="line">      D[i][<span class="number">0</span>]=sys.maxint</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,l2+<span class="number">1</span>):</span><br><span class="line">      D[<span class="number">0</span>][j]=sys.maxint</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,l1+<span class="number">1</span>):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,l2+<span class="number">1</span>):</span><br><span class="line">        D[i][j]=M[i-<span class="number">1</span>][j-<span class="number">1</span>]+<span class="built_in">min</span>(D[i-<span class="number">1</span>][j],D[i][j-<span class="number">1</span>],D[i-<span class="number">1</span>][j-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> D[l1][l2]</span><br></pre></td></tr></table></figure></p>
<p>__DTW采用动态规划实现，时间复杂度为<span
class="math inline">\(O(N^{2})\)</span>,有一些改进的快速DTW算法，如FastDTW[1],
SparseDTW, LB_Keogh, LB_Imporved等</p>
<h2 id="参考资料">参考资料</h2>
<ol type="1">
<li>FastDTW: Toward Accurate Dynamic Time Warping in Linear Time and
Space. Stan Salvador, Philip Chan.</li>
</ol>
]]></content>
      <categories>
        <category>时间序列分析</category>
      </categories>
      <tags>
        <tag>时间序列分析</tag>
      </tags>
  </entry>
</search>
